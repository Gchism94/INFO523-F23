{
  "hash": "8094b231cbc2c4f658f4e84bb2a32268",
  "result": {
    "markdown": "---\ntitle: \"INFO 523 Exercise\"\nsubtitle: \"Week 2: Data Preprocessing in R\"\nformat: \n  pdf:\n    toc: true\n    number-sections: false\n    colorlinks: true\n\neditor: visual\n---\n\n\n# Goal\n\nPractice basic R commands/methods for descriptive data analysis. If you are already familiar with some of the commands/methods, practice the ones new to you.\n\n**Note**: copying and pasting early in learning will not produce the results you are looking for, and will catch up to you eventually.\n\n## Submission\n\nPlease submit `.r`, `.rmd`, or `.qmd` files ONLY.\n\n# Installing required packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First run this\ninstall.packages(\"pacman\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\n\np_load(DBI, # DBI databases\n       dlookr,\n       here, # Reproducible/ standard directories\n       janitor,\n       RMySQL, # Utilizing MySQL drivers\n       tidymodels, # Tidyverse format modeling (e.g., lm())\n       tidyverse, # Data wrangling, manipulation, visualization\n       qqplotr) \n```\n:::\n\n\n# Loading data\n\n### CSV files (`.csv`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_csv(here(\"data\", \"x.csv\"))\n\ndata |> glimpse()\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nRows: 4 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Name\ndbl (2): ID, Age\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 4\nColumns: 3\n$ ID   <dbl> 12345, 12346, 12347, 12348\n$ Name <chr> \"Amy\", \"Ana\", \"June\", \"May\"\n$ Age  <dbl> 13, 14, 25, 21\n```\n:::\n:::\n\n\nThe `|>` is the Base R pipe as opposed to the `magrittr` pipe `|>`. The `|>` pipe can be utilized for most functions in R, while the `|>` pipe is more restricted towards the `tidyverse`.\n\n### Tab separated values (`x.tsv`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_delim(here(\"data\", \"x.tsv\"))\n\ndata |> glimpse()\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nRows: 4 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr (1): Name\ndbl (2): ID, Age\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 4\nColumns: 3\n$ ID   <dbl> 12345, 12346, 12347, 12348\n$ Name <chr> \"Amy\", \"Ana\", \"June\", \"May\"\n$ Age  <dbl> 13, 14, NA, 21\n```\n:::\n:::\n\n\n## Importing data from MySQL database\n\nFirst connect to a database in a MySQL database management system, then query tables in the database to obtain desired dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndrv <- dbDriver(\"MySQL\") #obtain the driver for MySQL, drivers available for other DBMS\n```\n:::\n\n\nGet a connection to my local mysql database `etcsite_charaparser`. If you don't have a local database handy, you can skip this exercise and just get the idea on what it takes to connect to a db.\n\n### Using `dplyr` instead\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"dbplyr\") #install but donâ€™t run library() on this dbplyr.\n```\n:::\n\n\n### Obtain a connection\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- src_mysql(\"etcsite_charaparser\", user = \"termsuser\", password = \"termspassword\", host = \"localhost\")\n```\n:::\n\n\nGet an entire table as `tbl`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nallwords <- tbl(con, \"1_allwords\")\nallwords\n```\n:::\n\n\n![](sqlDataOutput.png){fig-align=\"left\" width=\"500\"}\n\nNext you can use methods in dplyr to select and filter if needed. DBI library allows you to use SQL query to select your data from db tables while dplyr library methods can be used when you don't even know SQL language.\n\nImporting extremely large dataset, consider `data.table` package's `fread()`, or `iotools` package's `read.csv.raw()`.\n\n# Data Cleaning\n\n## Wide vs. long format\n\nRead data in wide format\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwide <- read_delim(here(\"data\", \"wide.txt\"), delim = \" \", skip = 1, col_names = c(\"Name\", \"Math\", \"English\", \"Degree_Year\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nRows: 3 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (2): Name, Degree_Year\ndbl (2): Math, English\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 4\n  Name   Math English Degree_Year\n  <chr> <dbl>   <dbl> <chr>      \n1 Anna     86      90 Bio_2014   \n2 John     43      75 Math_2013  \n3 Cath     80      82 Bio_2012   \n```\n:::\n:::\n\n\nThe wide format uses the values (`Math`, `English`) of variable `Subjects` as variables.\n\nThe long format should have `Name`, `Subject`, and `Grade` as variables (i.e., columns).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong <- wide |>\n  pivot_longer(cols = c(Math, English),\n               names_to = \"Subject\", \n               values_to = \"Grade\")\nlong\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  Name  Degree_Year Subject Grade\n  <chr> <chr>       <chr>   <dbl>\n1 Anna  Bio_2014    Math       86\n2 Anna  Bio_2014    English    90\n3 John  Math_2013   Math       43\n4 John  Math_2013   English    75\n5 Cath  Bio_2012    Math       80\n6 Cath  Bio_2012    English    82\n```\n:::\n:::\n\n\n## Long to wide, use `spread()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwide <- long %>%\n  pivot_wider(names_from = Subject, values_from = Grade)\nwide\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 4\n  Name  Degree_Year  Math English\n  <chr> <chr>       <dbl>   <dbl>\n1 Anna  Bio_2014       86      90\n2 John  Math_2013      43      75\n3 Cath  Bio_2012       80      82\n```\n:::\n:::\n\n\n## Split a column into multiple columns\n\nSplit `Degree_Year` to `Degree` and `Year`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclean <- long %>%\n  separate(Degree_Year, c(\"Degree\", \"Year\"), sep = \"_\")\n\nclean\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 5\n  Name  Degree Year  Subject Grade\n  <chr> <chr>  <chr> <chr>   <dbl>\n1 Anna  Bio    2014  Math       86\n2 Anna  Bio    2014  English    90\n3 John  Math   2013  Math       43\n4 John  Math   2013  English    75\n5 Cath  Bio    2012  Math       80\n6 Cath  Bio    2012  English    82\n```\n:::\n:::\n\n\n## Handling date/time and time zones\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"lubridate\")\nlibrary(lubridate)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nConvert dates of variance formats into one format:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmixed.dates <- c(20140123, \"2019-12-12\", \"2009/5/1\",\n \"measured on 2002-12-06\", \"2018-7/16\")\nclean.dates <- ymd(mixed.dates) #convert to year-month-day format\nclean.dates\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2014-01-23\" \"2019-12-12\" \"2009-05-01\" \"2002-12-06\" \"2018-07-16\"\n```\n:::\n:::\n\n\nExtract day, week, month, year info from dates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(Dates = clean.dates, WeekDay = wday(clean.dates), nWeekDay = wday(clean.dates, label = TRUE), Year = year(clean.dates), Month = month(clean.dates, label = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Dates WeekDay nWeekDay Year Month\n1 2014-01-23       5      Thu 2014   Jan\n2 2019-12-12       5      Thu 2019   Dec\n3 2009-05-01       6      Fri 2009   May\n4 2002-12-06       6      Fri 2002   Dec\n5 2018-07-16       2      Mon 2018   Jul\n```\n:::\n:::\n\n\nTime zone:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndate.time <- ymd_hms(\"20190203 03:00:03\", tz=\"Asia/Shanghai\")\n```\n:::\n\n\nConvert to Phoenix, AZ time:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith_tz(date.time, tz=\"America/Phoenix\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2019-02-02 12:00:03 MST\"\n```\n:::\n:::\n\n\nChange the timezone for a time:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforce_tz(date.time, \"Turkey\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2019-02-03 03:00:03 +03\"\n```\n:::\n:::\n\n\nCheck available time zones:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOlsonNames()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] \"Africa/Abidjan\"                   \"Africa/Accra\"                    \n  [3] \"Africa/Addis_Ababa\"               \"Africa/Algiers\"                  \n  [5] \"Africa/Asmara\"                    \"Africa/Asmera\"                   \n  [7] \"Africa/Bamako\"                    \"Africa/Bangui\"                   \n  [9] \"Africa/Banjul\"                    \"Africa/Bissau\"                   \n [11] \"Africa/Blantyre\"                  \"Africa/Brazzaville\"              \n [13] \"Africa/Bujumbura\"                 \"Africa/Cairo\"                    \n [15] \"Africa/Casablanca\"                \"Africa/Ceuta\"                    \n [17] \"Africa/Conakry\"                   \"Africa/Dakar\"                    \n [19] \"Africa/Dar_es_Salaam\"             \"Africa/Djibouti\"                 \n [21] \"Africa/Douala\"                    \"Africa/El_Aaiun\"                 \n [23] \"Africa/Freetown\"                  \"Africa/Gaborone\"                 \n [25] \"Africa/Harare\"                    \"Africa/Johannesburg\"             \n [27] \"Africa/Juba\"                      \"Africa/Kampala\"                  \n [29] \"Africa/Khartoum\"                  \"Africa/Kigali\"                   \n [31] \"Africa/Kinshasa\"                  \"Africa/Lagos\"                    \n [33] \"Africa/Libreville\"                \"Africa/Lome\"                     \n [35] \"Africa/Luanda\"                    \"Africa/Lubumbashi\"               \n [37] \"Africa/Lusaka\"                    \"Africa/Malabo\"                   \n [39] \"Africa/Maputo\"                    \"Africa/Maseru\"                   \n [41] \"Africa/Mbabane\"                   \"Africa/Mogadishu\"                \n [43] \"Africa/Monrovia\"                  \"Africa/Nairobi\"                  \n [45] \"Africa/Ndjamena\"                  \"Africa/Niamey\"                   \n [47] \"Africa/Nouakchott\"                \"Africa/Ouagadougou\"              \n [49] \"Africa/Porto-Novo\"                \"Africa/Sao_Tome\"                 \n [51] \"Africa/Timbuktu\"                  \"Africa/Tripoli\"                  \n [53] \"Africa/Tunis\"                     \"Africa/Windhoek\"                 \n [55] \"America/Adak\"                     \"America/Anchorage\"               \n [57] \"America/Anguilla\"                 \"America/Antigua\"                 \n [59] \"America/Araguaina\"                \"America/Argentina/Buenos_Aires\"  \n [61] \"America/Argentina/Catamarca\"      \"America/Argentina/ComodRivadavia\"\n [63] \"America/Argentina/Cordoba\"        \"America/Argentina/Jujuy\"         \n [65] \"America/Argentina/La_Rioja\"       \"America/Argentina/Mendoza\"       \n [67] \"America/Argentina/Rio_Gallegos\"   \"America/Argentina/Salta\"         \n [69] \"America/Argentina/San_Juan\"       \"America/Argentina/San_Luis\"      \n [71] \"America/Argentina/Tucuman\"        \"America/Argentina/Ushuaia\"       \n [73] \"America/Aruba\"                    \"America/Asuncion\"                \n [75] \"America/Atikokan\"                 \"America/Atka\"                    \n [77] \"America/Bahia\"                    \"America/Bahia_Banderas\"          \n [79] \"America/Barbados\"                 \"America/Belem\"                   \n [81] \"America/Belize\"                   \"America/Blanc-Sablon\"            \n [83] \"America/Boa_Vista\"                \"America/Bogota\"                  \n [85] \"America/Boise\"                    \"America/Buenos_Aires\"            \n [87] \"America/Cambridge_Bay\"            \"America/Campo_Grande\"            \n [89] \"America/Cancun\"                   \"America/Caracas\"                 \n [91] \"America/Catamarca\"                \"America/Cayenne\"                 \n [93] \"America/Cayman\"                   \"America/Chicago\"                 \n [95] \"America/Chihuahua\"                \"America/Ciudad_Juarez\"           \n [97] \"America/Coral_Harbour\"            \"America/Cordoba\"                 \n [99] \"America/Costa_Rica\"               \"America/Creston\"                 \n[101] \"America/Cuiaba\"                   \"America/Curacao\"                 \n[103] \"America/Danmarkshavn\"             \"America/Dawson\"                  \n[105] \"America/Dawson_Creek\"             \"America/Denver\"                  \n[107] \"America/Detroit\"                  \"America/Dominica\"                \n[109] \"America/Edmonton\"                 \"America/Eirunepe\"                \n[111] \"America/El_Salvador\"              \"America/Ensenada\"                \n[113] \"America/Fort_Nelson\"              \"America/Fort_Wayne\"              \n[115] \"America/Fortaleza\"                \"America/Glace_Bay\"               \n[117] \"America/Godthab\"                  \"America/Goose_Bay\"               \n[119] \"America/Grand_Turk\"               \"America/Grenada\"                 \n[121] \"America/Guadeloupe\"               \"America/Guatemala\"               \n[123] \"America/Guayaquil\"                \"America/Guyana\"                  \n[125] \"America/Halifax\"                  \"America/Havana\"                  \n[127] \"America/Hermosillo\"               \"America/Indiana/Indianapolis\"    \n[129] \"America/Indiana/Knox\"             \"America/Indiana/Marengo\"         \n[131] \"America/Indiana/Petersburg\"       \"America/Indiana/Tell_City\"       \n[133] \"America/Indiana/Vevay\"            \"America/Indiana/Vincennes\"       \n[135] \"America/Indiana/Winamac\"          \"America/Indianapolis\"            \n[137] \"America/Inuvik\"                   \"America/Iqaluit\"                 \n[139] \"America/Jamaica\"                  \"America/Jujuy\"                   \n[141] \"America/Juneau\"                   \"America/Kentucky/Louisville\"     \n[143] \"America/Kentucky/Monticello\"      \"America/Knox_IN\"                 \n[145] \"America/Kralendijk\"               \"America/La_Paz\"                  \n[147] \"America/Lima\"                     \"America/Los_Angeles\"             \n[149] \"America/Louisville\"               \"America/Lower_Princes\"           \n[151] \"America/Maceio\"                   \"America/Managua\"                 \n[153] \"America/Manaus\"                   \"America/Marigot\"                 \n[155] \"America/Martinique\"               \"America/Matamoros\"               \n[157] \"America/Mazatlan\"                 \"America/Mendoza\"                 \n[159] \"America/Menominee\"                \"America/Merida\"                  \n[161] \"America/Metlakatla\"               \"America/Mexico_City\"             \n[163] \"America/Miquelon\"                 \"America/Moncton\"                 \n[165] \"America/Monterrey\"                \"America/Montevideo\"              \n[167] \"America/Montreal\"                 \"America/Montserrat\"              \n[169] \"America/Nassau\"                   \"America/New_York\"                \n[171] \"America/Nipigon\"                  \"America/Nome\"                    \n[173] \"America/Noronha\"                  \"America/North_Dakota/Beulah\"     \n[175] \"America/North_Dakota/Center\"      \"America/North_Dakota/New_Salem\"  \n[177] \"America/Nuuk\"                     \"America/Ojinaga\"                 \n[179] \"America/Panama\"                   \"America/Pangnirtung\"             \n[181] \"America/Paramaribo\"               \"America/Phoenix\"                 \n[183] \"America/Port_of_Spain\"            \"America/Port-au-Prince\"          \n[185] \"America/Porto_Acre\"               \"America/Porto_Velho\"             \n[187] \"America/Puerto_Rico\"              \"America/Punta_Arenas\"            \n[189] \"America/Rainy_River\"              \"America/Rankin_Inlet\"            \n[191] \"America/Recife\"                   \"America/Regina\"                  \n[193] \"America/Resolute\"                 \"America/Rio_Branco\"              \n[195] \"America/Rosario\"                  \"America/Santa_Isabel\"            \n[197] \"America/Santarem\"                 \"America/Santiago\"                \n[199] \"America/Santo_Domingo\"            \"America/Sao_Paulo\"               \n[201] \"America/Scoresbysund\"             \"America/Shiprock\"                \n[203] \"America/Sitka\"                    \"America/St_Barthelemy\"           \n[205] \"America/St_Johns\"                 \"America/St_Kitts\"                \n[207] \"America/St_Lucia\"                 \"America/St_Thomas\"               \n[209] \"America/St_Vincent\"               \"America/Swift_Current\"           \n[211] \"America/Tegucigalpa\"              \"America/Thule\"                   \n[213] \"America/Thunder_Bay\"              \"America/Tijuana\"                 \n[215] \"America/Toronto\"                  \"America/Tortola\"                 \n[217] \"America/Vancouver\"                \"America/Virgin\"                  \n[219] \"America/Whitehorse\"               \"America/Winnipeg\"                \n[221] \"America/Yakutat\"                  \"America/Yellowknife\"             \n[223] \"Antarctica/Casey\"                 \"Antarctica/Davis\"                \n[225] \"Antarctica/DumontDUrville\"        \"Antarctica/Macquarie\"            \n[227] \"Antarctica/Mawson\"                \"Antarctica/McMurdo\"              \n[229] \"Antarctica/Palmer\"                \"Antarctica/Rothera\"              \n[231] \"Antarctica/South_Pole\"            \"Antarctica/Syowa\"                \n[233] \"Antarctica/Troll\"                 \"Antarctica/Vostok\"               \n[235] \"Arctic/Longyearbyen\"              \"Asia/Aden\"                       \n[237] \"Asia/Almaty\"                      \"Asia/Amman\"                      \n[239] \"Asia/Anadyr\"                      \"Asia/Aqtau\"                      \n[241] \"Asia/Aqtobe\"                      \"Asia/Ashgabat\"                   \n[243] \"Asia/Ashkhabad\"                   \"Asia/Atyrau\"                     \n[245] \"Asia/Baghdad\"                     \"Asia/Bahrain\"                    \n[247] \"Asia/Baku\"                        \"Asia/Bangkok\"                    \n[249] \"Asia/Barnaul\"                     \"Asia/Beirut\"                     \n[251] \"Asia/Bishkek\"                     \"Asia/Brunei\"                     \n[253] \"Asia/Calcutta\"                    \"Asia/Chita\"                      \n[255] \"Asia/Choibalsan\"                  \"Asia/Chongqing\"                  \n[257] \"Asia/Chungking\"                   \"Asia/Colombo\"                    \n[259] \"Asia/Dacca\"                       \"Asia/Damascus\"                   \n[261] \"Asia/Dhaka\"                       \"Asia/Dili\"                       \n[263] \"Asia/Dubai\"                       \"Asia/Dushanbe\"                   \n[265] \"Asia/Famagusta\"                   \"Asia/Gaza\"                       \n[267] \"Asia/Harbin\"                      \"Asia/Hebron\"                     \n[269] \"Asia/Ho_Chi_Minh\"                 \"Asia/Hong_Kong\"                  \n[271] \"Asia/Hovd\"                        \"Asia/Irkutsk\"                    \n[273] \"Asia/Istanbul\"                    \"Asia/Jakarta\"                    \n[275] \"Asia/Jayapura\"                    \"Asia/Jerusalem\"                  \n[277] \"Asia/Kabul\"                       \"Asia/Kamchatka\"                  \n[279] \"Asia/Karachi\"                     \"Asia/Kashgar\"                    \n[281] \"Asia/Kathmandu\"                   \"Asia/Katmandu\"                   \n[283] \"Asia/Khandyga\"                    \"Asia/Kolkata\"                    \n[285] \"Asia/Krasnoyarsk\"                 \"Asia/Kuala_Lumpur\"               \n[287] \"Asia/Kuching\"                     \"Asia/Kuwait\"                     \n[289] \"Asia/Macao\"                       \"Asia/Macau\"                      \n[291] \"Asia/Magadan\"                     \"Asia/Makassar\"                   \n[293] \"Asia/Manila\"                      \"Asia/Muscat\"                     \n[295] \"Asia/Nicosia\"                     \"Asia/Novokuznetsk\"               \n[297] \"Asia/Novosibirsk\"                 \"Asia/Omsk\"                       \n[299] \"Asia/Oral\"                        \"Asia/Phnom_Penh\"                 \n[301] \"Asia/Pontianak\"                   \"Asia/Pyongyang\"                  \n[303] \"Asia/Qatar\"                       \"Asia/Qostanay\"                   \n[305] \"Asia/Qyzylorda\"                   \"Asia/Rangoon\"                    \n[307] \"Asia/Riyadh\"                      \"Asia/Saigon\"                     \n[309] \"Asia/Sakhalin\"                    \"Asia/Samarkand\"                  \n[311] \"Asia/Seoul\"                       \"Asia/Shanghai\"                   \n[313] \"Asia/Singapore\"                   \"Asia/Srednekolymsk\"              \n[315] \"Asia/Taipei\"                      \"Asia/Tashkent\"                   \n[317] \"Asia/Tbilisi\"                     \"Asia/Tehran\"                     \n[319] \"Asia/Tel_Aviv\"                    \"Asia/Thimbu\"                     \n[321] \"Asia/Thimphu\"                     \"Asia/Tokyo\"                      \n[323] \"Asia/Tomsk\"                       \"Asia/Ujung_Pandang\"              \n[325] \"Asia/Ulaanbaatar\"                 \"Asia/Ulan_Bator\"                 \n[327] \"Asia/Urumqi\"                      \"Asia/Ust-Nera\"                   \n[329] \"Asia/Vientiane\"                   \"Asia/Vladivostok\"                \n[331] \"Asia/Yakutsk\"                     \"Asia/Yangon\"                     \n[333] \"Asia/Yekaterinburg\"               \"Asia/Yerevan\"                    \n[335] \"Atlantic/Azores\"                  \"Atlantic/Bermuda\"                \n[337] \"Atlantic/Canary\"                  \"Atlantic/Cape_Verde\"             \n[339] \"Atlantic/Faeroe\"                  \"Atlantic/Faroe\"                  \n[341] \"Atlantic/Jan_Mayen\"               \"Atlantic/Madeira\"                \n[343] \"Atlantic/Reykjavik\"               \"Atlantic/South_Georgia\"          \n[345] \"Atlantic/St_Helena\"               \"Atlantic/Stanley\"                \n[347] \"Australia/ACT\"                    \"Australia/Adelaide\"              \n[349] \"Australia/Brisbane\"               \"Australia/Broken_Hill\"           \n[351] \"Australia/Canberra\"               \"Australia/Currie\"                \n[353] \"Australia/Darwin\"                 \"Australia/Eucla\"                 \n[355] \"Australia/Hobart\"                 \"Australia/LHI\"                   \n[357] \"Australia/Lindeman\"               \"Australia/Lord_Howe\"             \n[359] \"Australia/Melbourne\"              \"Australia/North\"                 \n[361] \"Australia/NSW\"                    \"Australia/Perth\"                 \n[363] \"Australia/Queensland\"             \"Australia/South\"                 \n[365] \"Australia/Sydney\"                 \"Australia/Tasmania\"              \n[367] \"Australia/Victoria\"               \"Australia/West\"                  \n[369] \"Australia/Yancowinna\"             \"Brazil/Acre\"                     \n[371] \"Brazil/DeNoronha\"                 \"Brazil/East\"                     \n[373] \"Brazil/West\"                      \"Canada/Atlantic\"                 \n[375] \"Canada/Central\"                   \"Canada/Eastern\"                  \n[377] \"Canada/Mountain\"                  \"Canada/Newfoundland\"             \n[379] \"Canada/Pacific\"                   \"Canada/Saskatchewan\"             \n[381] \"Canada/Yukon\"                     \"CET\"                             \n[383] \"Chile/Continental\"                \"Chile/EasterIsland\"              \n[385] \"CST6CDT\"                          \"Cuba\"                            \n[387] \"EET\"                              \"Egypt\"                           \n[389] \"Eire\"                             \"EST\"                             \n[391] \"EST5EDT\"                          \"Etc/GMT\"                         \n[393] \"Etc/GMT-0\"                        \"Etc/GMT-1\"                       \n[395] \"Etc/GMT-10\"                       \"Etc/GMT-11\"                      \n[397] \"Etc/GMT-12\"                       \"Etc/GMT-13\"                      \n[399] \"Etc/GMT-14\"                       \"Etc/GMT-2\"                       \n[401] \"Etc/GMT-3\"                        \"Etc/GMT-4\"                       \n[403] \"Etc/GMT-5\"                        \"Etc/GMT-6\"                       \n[405] \"Etc/GMT-7\"                        \"Etc/GMT-8\"                       \n[407] \"Etc/GMT-9\"                        \"Etc/GMT+0\"                       \n[409] \"Etc/GMT+1\"                        \"Etc/GMT+10\"                      \n[411] \"Etc/GMT+11\"                       \"Etc/GMT+12\"                      \n[413] \"Etc/GMT+2\"                        \"Etc/GMT+3\"                       \n[415] \"Etc/GMT+4\"                        \"Etc/GMT+5\"                       \n[417] \"Etc/GMT+6\"                        \"Etc/GMT+7\"                       \n[419] \"Etc/GMT+8\"                        \"Etc/GMT+9\"                       \n[421] \"Etc/GMT0\"                         \"Etc/Greenwich\"                   \n[423] \"Etc/UCT\"                          \"Etc/Universal\"                   \n[425] \"Etc/UTC\"                          \"Etc/Zulu\"                        \n[427] \"Europe/Amsterdam\"                 \"Europe/Andorra\"                  \n[429] \"Europe/Astrakhan\"                 \"Europe/Athens\"                   \n[431] \"Europe/Belfast\"                   \"Europe/Belgrade\"                 \n[433] \"Europe/Berlin\"                    \"Europe/Bratislava\"               \n[435] \"Europe/Brussels\"                  \"Europe/Bucharest\"                \n[437] \"Europe/Budapest\"                  \"Europe/Busingen\"                 \n[439] \"Europe/Chisinau\"                  \"Europe/Copenhagen\"               \n[441] \"Europe/Dublin\"                    \"Europe/Gibraltar\"                \n[443] \"Europe/Guernsey\"                  \"Europe/Helsinki\"                 \n[445] \"Europe/Isle_of_Man\"               \"Europe/Istanbul\"                 \n[447] \"Europe/Jersey\"                    \"Europe/Kaliningrad\"              \n[449] \"Europe/Kiev\"                      \"Europe/Kirov\"                    \n[451] \"Europe/Kyiv\"                      \"Europe/Lisbon\"                   \n[453] \"Europe/Ljubljana\"                 \"Europe/London\"                   \n[455] \"Europe/Luxembourg\"                \"Europe/Madrid\"                   \n[457] \"Europe/Malta\"                     \"Europe/Mariehamn\"                \n[459] \"Europe/Minsk\"                     \"Europe/Monaco\"                   \n[461] \"Europe/Moscow\"                    \"Europe/Nicosia\"                  \n[463] \"Europe/Oslo\"                      \"Europe/Paris\"                    \n[465] \"Europe/Podgorica\"                 \"Europe/Prague\"                   \n[467] \"Europe/Riga\"                      \"Europe/Rome\"                     \n[469] \"Europe/Samara\"                    \"Europe/San_Marino\"               \n[471] \"Europe/Sarajevo\"                  \"Europe/Saratov\"                  \n[473] \"Europe/Simferopol\"                \"Europe/Skopje\"                   \n[475] \"Europe/Sofia\"                     \"Europe/Stockholm\"                \n[477] \"Europe/Tallinn\"                   \"Europe/Tirane\"                   \n[479] \"Europe/Tiraspol\"                  \"Europe/Ulyanovsk\"                \n[481] \"Europe/Uzhgorod\"                  \"Europe/Vaduz\"                    \n[483] \"Europe/Vatican\"                   \"Europe/Vienna\"                   \n[485] \"Europe/Vilnius\"                   \"Europe/Volgograd\"                \n[487] \"Europe/Warsaw\"                    \"Europe/Zagreb\"                   \n[489] \"Europe/Zaporozhye\"                \"Europe/Zurich\"                   \n[491] \"Factory\"                          \"GB\"                              \n[493] \"GB-Eire\"                          \"GMT\"                             \n[495] \"GMT-0\"                            \"GMT+0\"                           \n[497] \"GMT0\"                             \"Greenwich\"                       \n[499] \"Hongkong\"                         \"HST\"                             \n[501] \"Iceland\"                          \"Indian/Antananarivo\"             \n[503] \"Indian/Chagos\"                    \"Indian/Christmas\"                \n[505] \"Indian/Cocos\"                     \"Indian/Comoro\"                   \n[507] \"Indian/Kerguelen\"                 \"Indian/Mahe\"                     \n[509] \"Indian/Maldives\"                  \"Indian/Mauritius\"                \n[511] \"Indian/Mayotte\"                   \"Indian/Reunion\"                  \n[513] \"Iran\"                             \"Israel\"                          \n[515] \"Jamaica\"                          \"Japan\"                           \n[517] \"Kwajalein\"                        \"Libya\"                           \n[519] \"MET\"                              \"Mexico/BajaNorte\"                \n[521] \"Mexico/BajaSur\"                   \"Mexico/General\"                  \n[523] \"MST\"                              \"MST7MDT\"                         \n[525] \"Navajo\"                           \"NZ\"                              \n[527] \"NZ-CHAT\"                          \"Pacific/Apia\"                    \n[529] \"Pacific/Auckland\"                 \"Pacific/Bougainville\"            \n[531] \"Pacific/Chatham\"                  \"Pacific/Chuuk\"                   \n[533] \"Pacific/Easter\"                   \"Pacific/Efate\"                   \n[535] \"Pacific/Enderbury\"                \"Pacific/Fakaofo\"                 \n[537] \"Pacific/Fiji\"                     \"Pacific/Funafuti\"                \n[539] \"Pacific/Galapagos\"                \"Pacific/Gambier\"                 \n[541] \"Pacific/Guadalcanal\"              \"Pacific/Guam\"                    \n[543] \"Pacific/Honolulu\"                 \"Pacific/Johnston\"                \n[545] \"Pacific/Kanton\"                   \"Pacific/Kiritimati\"              \n[547] \"Pacific/Kosrae\"                   \"Pacific/Kwajalein\"               \n[549] \"Pacific/Majuro\"                   \"Pacific/Marquesas\"               \n[551] \"Pacific/Midway\"                   \"Pacific/Nauru\"                   \n[553] \"Pacific/Niue\"                     \"Pacific/Norfolk\"                 \n[555] \"Pacific/Noumea\"                   \"Pacific/Pago_Pago\"               \n[557] \"Pacific/Palau\"                    \"Pacific/Pitcairn\"                \n[559] \"Pacific/Pohnpei\"                  \"Pacific/Ponape\"                  \n[561] \"Pacific/Port_Moresby\"             \"Pacific/Rarotonga\"               \n[563] \"Pacific/Saipan\"                   \"Pacific/Samoa\"                   \n[565] \"Pacific/Tahiti\"                   \"Pacific/Tarawa\"                  \n[567] \"Pacific/Tongatapu\"                \"Pacific/Truk\"                    \n[569] \"Pacific/Wake\"                     \"Pacific/Wallis\"                  \n[571] \"Pacific/Yap\"                      \"Poland\"                          \n[573] \"Portugal\"                         \"PRC\"                             \n[575] \"PST8PDT\"                          \"ROC\"                             \n[577] \"ROK\"                              \"Singapore\"                       \n[579] \"Turkey\"                           \"UCT\"                             \n[581] \"Universal\"                        \"US/Alaska\"                       \n[583] \"US/Aleutian\"                      \"US/Arizona\"                      \n[585] \"US/Central\"                       \"US/East-Indiana\"                 \n[587] \"US/Eastern\"                       \"US/Hawaii\"                       \n[589] \"US/Indiana-Starke\"                \"US/Michigan\"                     \n[591] \"US/Mountain\"                      \"US/Pacific\"                      \n[593] \"US/Samoa\"                         \"UTC\"                             \n[595] \"W-SU\"                             \"WET\"                             \n[597] \"Zulu\"                            \nattr(,\"Version\")\n[1] \"2023c\"\n```\n:::\n:::\n\n\n## String Processing\n\nCommon needs: `stringr` package\n\nAdvanced needs: `stringi` package\n\nThe following code shows the use of functions provided by stringr package to put column names back to a dataset fetched from <http://archive.ics.uci.edu/ml/machine-learning-databases/audiology/>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\n```\n:::\n\n\nFetch data from a URL, form the URL using string functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuci.repo <-\"http://archive.ics.uci.edu/ml/machine-learning-databases/\"\n\ndataset <- \"audiology/audiology.standardized\"\n```\n:::\n\n\n`str_c`: string concatenation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataF <- str_c(uci.repo, dataset, \".data\")\nnamesF <- str_c(uci.repo, dataset, \".names\")\ndataF\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"http://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\"\n```\n:::\n:::\n\n\nRead the data file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_csv(url(dataF), col_names = FALSE, na=\"?\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 200 Columns: 71\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (11): X2, X4, X5, X6, X8, X59, X60, X64, X66, X70, X71\nlgl (60): X1, X3, X7, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, ...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 200  71\n```\n:::\n:::\n\n\nRead the name file line by line, put the lines in a vector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlines <- read_lines(url(namesF))\n\nlines |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: This database should be credited to the original owner whenever\"\n[2] \"         used for any publication whatsoever.\"                           \n[3] \"\"                                                                        \n[4] \"1. Title: Standardized Audiology Database \"                              \n[5] \"\"                                                                        \n[6] \"2. Sources:\"                                                             \n```\n:::\n:::\n\n\nExamine the content of lines and see the column names start on line 67, ends on line 135. Then, get column name lines and clean up to get column names:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames <- lines[67:135]\nnames\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"   age_gt_60:\\t\\t     f, t.\"                                          \n [2] \"   air():\\t\\t     mild,moderate,severe,normal,profound.\"              \n [3] \"   airBoneGap:\\t\\t     f, t.\"                                         \n [4] \"   ar_c():\\t\\t     normal,elevated,absent.\"                           \n [5] \"   ar_u():\\t\\t     normal,absent,elevated.\"                           \n [6] \"   bone():\\t\\t     mild,moderate,normal,unmeasured.\"                  \n [7] \"   boneAbnormal:\\t     f, t.\"                                         \n [8] \"   bser():\\t\\t     normal,degraded.\"                                  \n [9] \"   history_buzzing:\\t     f, t.\"                                      \n[10] \"   history_dizziness:\\t     f, t.\"                                    \n[11] \"   history_fluctuating:\\t     f, t.\"                                  \n[12] \"   history_fullness:\\t     f, t.\"                                     \n[13] \"   history_heredity:\\t     f, t.\"                                     \n[14] \"   history_nausea:\\t     f, t.\"                                       \n[15] \"   history_noise:\\t     f, t.\"                                        \n[16] \"   history_recruitment:\\t     f, t.\"                                  \n[17] \"   history_ringing:\\t     f, t.\"                                      \n[18] \"   history_roaring:\\t     f, t.\"                                      \n[19] \"   history_vomiting:\\t     f, t.\"                                     \n[20] \"   late_wave_poor:\\t     f, t.\"                                       \n[21] \"   m_at_2k:\\t\\t     f, t.\"                                            \n[22] \"   m_cond_lt_1k:\\t     f, t.\"                                         \n[23] \"   m_gt_1k:\\t\\t     f, t.\"                                            \n[24] \"   m_m_gt_2k:\\t\\t     f, t.\"                                          \n[25] \"   m_m_sn:\\t\\t     f, t.\"                                             \n[26] \"   m_m_sn_gt_1k:\\t     f, t.\"                                         \n[27] \"   m_m_sn_gt_2k:\\t     f, t.\"                                         \n[28] \"   m_m_sn_gt_500:\\t     f, t.\"                                        \n[29] \"   m_p_sn_gt_2k:\\t     f, t.\"                                         \n[30] \"   m_s_gt_500:\\t\\t     f, t.\"                                         \n[31] \"   m_s_sn:\\t\\t     f, t.\"                                             \n[32] \"   m_s_sn_gt_1k:\\t     f, t.\"                                         \n[33] \"   m_s_sn_gt_2k:\\t     f, t.\"                                         \n[34] \"   m_s_sn_gt_3k:\\t     f, t.\"                                         \n[35] \"   m_s_sn_gt_4k:\\t     f, t.\"                                         \n[36] \"   m_sn_2_3k:\\t\\t     f, t.\"                                          \n[37] \"   m_sn_gt_1k:\\t\\t     f, t.\"                                         \n[38] \"   m_sn_gt_2k:\\t\\t     f, t.\"                                         \n[39] \"   m_sn_gt_3k:\\t\\t     f, t.\"                                         \n[40] \"   m_sn_gt_4k:\\t\\t     f, t.\"                                         \n[41] \"   m_sn_gt_500:\\t\\t     f, t.\"                                        \n[42] \"   m_sn_gt_6k:\\t\\t     f, t.\"                                         \n[43] \"   m_sn_lt_1k:\\t\\t     f, t. \"                                        \n[44] \"   m_sn_lt_2k:\\t\\t     f, t.\"                                         \n[45] \"   m_sn_lt_3k:\\t\\t     f, t.\"                                         \n[46] \"   middle_wave_poor:\\t     f, t.\"                                     \n[47] \"   mod_gt_4k:\\t\\t     f, t.\"                                          \n[48] \"   mod_mixed:\\t\\t     f, t.\"                                          \n[49] \"   mod_s_mixed:\\t\\t     f, t.\"                                        \n[50] \"   mod_s_sn_gt_500:\\t     f, t.\"                                      \n[51] \"   mod_sn:\\t\\t     f, t.\"                                             \n[52] \"   mod_sn_gt_1k:\\t     f, t.\"                                         \n[53] \"   mod_sn_gt_2k:\\t     f, t.\"                                         \n[54] \"   mod_sn_gt_3k:\\t     f, t.\"                                         \n[55] \"   mod_sn_gt_4k:\\t     f, t.\"                                         \n[56] \"   mod_sn_gt_500:\\t     f, t.\"                                        \n[57] \"   notch_4k:\\t\\t     f, t.\"                                           \n[58] \"   notch_at_4k:\\t\\t     f, t.\"                                        \n[59] \"   o_ar_c():\\t\\t     normal,elevated,absent.\"                         \n[60] \"   o_ar_u():\\t\\t     normal,absent,elevated.\"                         \n[61] \"   s_sn_gt_1k:\\t\\t     f, t.\"                                         \n[62] \"   s_sn_gt_2k:\\t\\t     f, t.\"                                         \n[63] \"   s_sn_gt_4k:\\t\\t     f, t.\"                                         \n[64] \"   speech():\\t\\t     normal,good,very_good,very_poor,poor,unmeasured.\"\n[65] \"   static_normal:\\t     f, t.\"                                        \n[66] \"   tymp():\\t\\t     a,as,b,ad,c.\"                                      \n[67] \"   viith_nerve_signs:        f, t.\"                                   \n[68] \"   wave_V_delayed:\\t     f, t.\"                                       \n[69] \"   waveform_ItoV_prolonged:  f, t.\"                                   \n```\n:::\n:::\n\n\nObserve: a name line consists two parts, name: valid values. The part before `:` is the name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames <- str_split_fixed(names, \":\", 2) #split on regular expression pattern \":\", this function returns a matrix\nnames\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]                        \n [1,] \"   age_gt_60\"              \n [2,] \"   air()\"                  \n [3,] \"   airBoneGap\"             \n [4,] \"   ar_c()\"                 \n [5,] \"   ar_u()\"                 \n [6,] \"   bone()\"                 \n [7,] \"   boneAbnormal\"           \n [8,] \"   bser()\"                 \n [9,] \"   history_buzzing\"        \n[10,] \"   history_dizziness\"      \n[11,] \"   history_fluctuating\"    \n[12,] \"   history_fullness\"       \n[13,] \"   history_heredity\"       \n[14,] \"   history_nausea\"         \n[15,] \"   history_noise\"          \n[16,] \"   history_recruitment\"    \n[17,] \"   history_ringing\"        \n[18,] \"   history_roaring\"        \n[19,] \"   history_vomiting\"       \n[20,] \"   late_wave_poor\"         \n[21,] \"   m_at_2k\"                \n[22,] \"   m_cond_lt_1k\"           \n[23,] \"   m_gt_1k\"                \n[24,] \"   m_m_gt_2k\"              \n[25,] \"   m_m_sn\"                 \n[26,] \"   m_m_sn_gt_1k\"           \n[27,] \"   m_m_sn_gt_2k\"           \n[28,] \"   m_m_sn_gt_500\"          \n[29,] \"   m_p_sn_gt_2k\"           \n[30,] \"   m_s_gt_500\"             \n[31,] \"   m_s_sn\"                 \n[32,] \"   m_s_sn_gt_1k\"           \n[33,] \"   m_s_sn_gt_2k\"           \n[34,] \"   m_s_sn_gt_3k\"           \n[35,] \"   m_s_sn_gt_4k\"           \n[36,] \"   m_sn_2_3k\"              \n[37,] \"   m_sn_gt_1k\"             \n[38,] \"   m_sn_gt_2k\"             \n[39,] \"   m_sn_gt_3k\"             \n[40,] \"   m_sn_gt_4k\"             \n[41,] \"   m_sn_gt_500\"            \n[42,] \"   m_sn_gt_6k\"             \n[43,] \"   m_sn_lt_1k\"             \n[44,] \"   m_sn_lt_2k\"             \n[45,] \"   m_sn_lt_3k\"             \n[46,] \"   middle_wave_poor\"       \n[47,] \"   mod_gt_4k\"              \n[48,] \"   mod_mixed\"              \n[49,] \"   mod_s_mixed\"            \n[50,] \"   mod_s_sn_gt_500\"        \n[51,] \"   mod_sn\"                 \n[52,] \"   mod_sn_gt_1k\"           \n[53,] \"   mod_sn_gt_2k\"           \n[54,] \"   mod_sn_gt_3k\"           \n[55,] \"   mod_sn_gt_4k\"           \n[56,] \"   mod_sn_gt_500\"          \n[57,] \"   notch_4k\"               \n[58,] \"   notch_at_4k\"            \n[59,] \"   o_ar_c()\"               \n[60,] \"   o_ar_u()\"               \n[61,] \"   s_sn_gt_1k\"             \n[62,] \"   s_sn_gt_2k\"             \n[63,] \"   s_sn_gt_4k\"             \n[64,] \"   speech()\"               \n[65,] \"   static_normal\"          \n[66,] \"   tymp()\"                 \n[67,] \"   viith_nerve_signs\"      \n[68,] \"   wave_V_delayed\"         \n[69,] \"   waveform_ItoV_prolonged\"\n      [,2]                                                       \n [1,] \"\\t\\t     f, t.\"                                           \n [2,] \"\\t\\t     mild,moderate,severe,normal,profound.\"           \n [3,] \"\\t\\t     f, t.\"                                           \n [4,] \"\\t\\t     normal,elevated,absent.\"                         \n [5,] \"\\t\\t     normal,absent,elevated.\"                         \n [6,] \"\\t\\t     mild,moderate,normal,unmeasured.\"                \n [7,] \"\\t     f, t.\"                                             \n [8,] \"\\t\\t     normal,degraded.\"                                \n [9,] \"\\t     f, t.\"                                             \n[10,] \"\\t     f, t.\"                                             \n[11,] \"\\t     f, t.\"                                             \n[12,] \"\\t     f, t.\"                                             \n[13,] \"\\t     f, t.\"                                             \n[14,] \"\\t     f, t.\"                                             \n[15,] \"\\t     f, t.\"                                             \n[16,] \"\\t     f, t.\"                                             \n[17,] \"\\t     f, t.\"                                             \n[18,] \"\\t     f, t.\"                                             \n[19,] \"\\t     f, t.\"                                             \n[20,] \"\\t     f, t.\"                                             \n[21,] \"\\t\\t     f, t.\"                                           \n[22,] \"\\t     f, t.\"                                             \n[23,] \"\\t\\t     f, t.\"                                           \n[24,] \"\\t\\t     f, t.\"                                           \n[25,] \"\\t\\t     f, t.\"                                           \n[26,] \"\\t     f, t.\"                                             \n[27,] \"\\t     f, t.\"                                             \n[28,] \"\\t     f, t.\"                                             \n[29,] \"\\t     f, t.\"                                             \n[30,] \"\\t\\t     f, t.\"                                           \n[31,] \"\\t\\t     f, t.\"                                           \n[32,] \"\\t     f, t.\"                                             \n[33,] \"\\t     f, t.\"                                             \n[34,] \"\\t     f, t.\"                                             \n[35,] \"\\t     f, t.\"                                             \n[36,] \"\\t\\t     f, t.\"                                           \n[37,] \"\\t\\t     f, t.\"                                           \n[38,] \"\\t\\t     f, t.\"                                           \n[39,] \"\\t\\t     f, t.\"                                           \n[40,] \"\\t\\t     f, t.\"                                           \n[41,] \"\\t\\t     f, t.\"                                           \n[42,] \"\\t\\t     f, t.\"                                           \n[43,] \"\\t\\t     f, t. \"                                          \n[44,] \"\\t\\t     f, t.\"                                           \n[45,] \"\\t\\t     f, t.\"                                           \n[46,] \"\\t     f, t.\"                                             \n[47,] \"\\t\\t     f, t.\"                                           \n[48,] \"\\t\\t     f, t.\"                                           \n[49,] \"\\t\\t     f, t.\"                                           \n[50,] \"\\t     f, t.\"                                             \n[51,] \"\\t\\t     f, t.\"                                           \n[52,] \"\\t     f, t.\"                                             \n[53,] \"\\t     f, t.\"                                             \n[54,] \"\\t     f, t.\"                                             \n[55,] \"\\t     f, t.\"                                             \n[56,] \"\\t     f, t.\"                                             \n[57,] \"\\t\\t     f, t.\"                                           \n[58,] \"\\t\\t     f, t.\"                                           \n[59,] \"\\t\\t     normal,elevated,absent.\"                         \n[60,] \"\\t\\t     normal,absent,elevated.\"                         \n[61,] \"\\t\\t     f, t.\"                                           \n[62,] \"\\t\\t     f, t.\"                                           \n[63,] \"\\t\\t     f, t.\"                                           \n[64,] \"\\t\\t     normal,good,very_good,very_poor,poor,unmeasured.\"\n[65,] \"\\t     f, t.\"                                             \n[66,] \"\\t\\t     a,as,b,ad,c.\"                                    \n[67,] \"        f, t.\"                                            \n[68,] \"\\t     f, t.\"                                             \n[69,] \"  f, t.\"                                                  \n```\n:::\n:::\n\n\nTake the first column, which contains names:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames <- names[,1]\nnames\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"   age_gt_60\"               \"   air()\"                  \n [3] \"   airBoneGap\"              \"   ar_c()\"                 \n [5] \"   ar_u()\"                  \"   bone()\"                 \n [7] \"   boneAbnormal\"            \"   bser()\"                 \n [9] \"   history_buzzing\"         \"   history_dizziness\"      \n[11] \"   history_fluctuating\"     \"   history_fullness\"       \n[13] \"   history_heredity\"        \"   history_nausea\"         \n[15] \"   history_noise\"           \"   history_recruitment\"    \n[17] \"   history_ringing\"         \"   history_roaring\"        \n[19] \"   history_vomiting\"        \"   late_wave_poor\"         \n[21] \"   m_at_2k\"                 \"   m_cond_lt_1k\"           \n[23] \"   m_gt_1k\"                 \"   m_m_gt_2k\"              \n[25] \"   m_m_sn\"                  \"   m_m_sn_gt_1k\"           \n[27] \"   m_m_sn_gt_2k\"            \"   m_m_sn_gt_500\"          \n[29] \"   m_p_sn_gt_2k\"            \"   m_s_gt_500\"             \n[31] \"   m_s_sn\"                  \"   m_s_sn_gt_1k\"           \n[33] \"   m_s_sn_gt_2k\"            \"   m_s_sn_gt_3k\"           \n[35] \"   m_s_sn_gt_4k\"            \"   m_sn_2_3k\"              \n[37] \"   m_sn_gt_1k\"              \"   m_sn_gt_2k\"             \n[39] \"   m_sn_gt_3k\"              \"   m_sn_gt_4k\"             \n[41] \"   m_sn_gt_500\"             \"   m_sn_gt_6k\"             \n[43] \"   m_sn_lt_1k\"              \"   m_sn_lt_2k\"             \n[45] \"   m_sn_lt_3k\"              \"   middle_wave_poor\"       \n[47] \"   mod_gt_4k\"               \"   mod_mixed\"              \n[49] \"   mod_s_mixed\"             \"   mod_s_sn_gt_500\"        \n[51] \"   mod_sn\"                  \"   mod_sn_gt_1k\"           \n[53] \"   mod_sn_gt_2k\"            \"   mod_sn_gt_3k\"           \n[55] \"   mod_sn_gt_4k\"            \"   mod_sn_gt_500\"          \n[57] \"   notch_4k\"                \"   notch_at_4k\"            \n[59] \"   o_ar_c()\"                \"   o_ar_u()\"               \n[61] \"   s_sn_gt_1k\"              \"   s_sn_gt_2k\"             \n[63] \"   s_sn_gt_4k\"              \"   speech()\"               \n[65] \"   static_normal\"           \"   tymp()\"                 \n[67] \"   viith_nerve_signs\"       \"   wave_V_delayed\"         \n[69] \"   waveform_ItoV_prolonged\"\n```\n:::\n:::\n\n\nNow clean up the names: trim spaces, remove `()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames <-str_trim(names) |> str_replace_all(\"\\\\(|\\\\)\", \"\") # we use a pipe, and another reg exp \"\\\\(|\\\\)\", \\\\ is the escape.\nnames\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"age_gt_60\"               \"air\"                    \n [3] \"airBoneGap\"              \"ar_c\"                   \n [5] \"ar_u\"                    \"bone\"                   \n [7] \"boneAbnormal\"            \"bser\"                   \n [9] \"history_buzzing\"         \"history_dizziness\"      \n[11] \"history_fluctuating\"     \"history_fullness\"       \n[13] \"history_heredity\"        \"history_nausea\"         \n[15] \"history_noise\"           \"history_recruitment\"    \n[17] \"history_ringing\"         \"history_roaring\"        \n[19] \"history_vomiting\"        \"late_wave_poor\"         \n[21] \"m_at_2k\"                 \"m_cond_lt_1k\"           \n[23] \"m_gt_1k\"                 \"m_m_gt_2k\"              \n[25] \"m_m_sn\"                  \"m_m_sn_gt_1k\"           \n[27] \"m_m_sn_gt_2k\"            \"m_m_sn_gt_500\"          \n[29] \"m_p_sn_gt_2k\"            \"m_s_gt_500\"             \n[31] \"m_s_sn\"                  \"m_s_sn_gt_1k\"           \n[33] \"m_s_sn_gt_2k\"            \"m_s_sn_gt_3k\"           \n[35] \"m_s_sn_gt_4k\"            \"m_sn_2_3k\"              \n[37] \"m_sn_gt_1k\"              \"m_sn_gt_2k\"             \n[39] \"m_sn_gt_3k\"              \"m_sn_gt_4k\"             \n[41] \"m_sn_gt_500\"             \"m_sn_gt_6k\"             \n[43] \"m_sn_lt_1k\"              \"m_sn_lt_2k\"             \n[45] \"m_sn_lt_3k\"              \"middle_wave_poor\"       \n[47] \"mod_gt_4k\"               \"mod_mixed\"              \n[49] \"mod_s_mixed\"             \"mod_s_sn_gt_500\"        \n[51] \"mod_sn\"                  \"mod_sn_gt_1k\"           \n[53] \"mod_sn_gt_2k\"            \"mod_sn_gt_3k\"           \n[55] \"mod_sn_gt_4k\"            \"mod_sn_gt_500\"          \n[57] \"notch_4k\"                \"notch_at_4k\"            \n[59] \"o_ar_c\"                  \"o_ar_u\"                 \n[61] \"s_sn_gt_1k\"              \"s_sn_gt_2k\"             \n[63] \"s_sn_gt_4k\"              \"speech\"                 \n[65] \"static_normal\"           \"tymp\"                   \n[67] \"viith_nerve_signs\"       \"wave_V_delayed\"         \n[69] \"waveform_ItoV_prolonged\"\n```\n:::\n:::\n\n\nFinally, put the columns to the data:\n\n*Note:* data has 71 rows but we only has 69 names. The last two columns in data are identifier and class labels. So we will put the 69 names to the first 69 columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(data)[1:69] <- names\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 71\n   age_gt_60 air      airBoneGap ar_c     ar_u   bone  boneAbnormal bser \n   <lgl>     <chr>    <lgl>      <chr>    <chr>  <chr> <lgl>        <chr>\n 1 FALSE     mild     FALSE      normal   normal <NA>  TRUE         <NA> \n 2 FALSE     moderate FALSE      normal   normal <NA>  TRUE         <NA> \n 3 TRUE      mild     TRUE       <NA>     absent mild  TRUE         <NA> \n 4 TRUE      mild     TRUE       <NA>     absent mild  FALSE        <NA> \n 5 TRUE      mild     FALSE      normal   normal mild  TRUE         <NA> \n 6 TRUE      mild     FALSE      normal   normal mild  TRUE         <NA> \n 7 FALSE     mild     FALSE      normal   normal mild  TRUE         <NA> \n 8 FALSE     mild     FALSE      normal   normal mild  TRUE         <NA> \n 9 FALSE     severe   FALSE      <NA>     <NA>   <NA>  TRUE         <NA> \n10 TRUE      mild     FALSE      elevated absent mild  TRUE         <NA> \n# i 190 more rows\n# i 63 more variables: history_buzzing <lgl>, history_dizziness <lgl>,\n#   history_fluctuating <lgl>, history_fullness <lgl>, history_heredity <lgl>,\n#   history_nausea <lgl>, history_noise <lgl>, history_recruitment <lgl>,\n#   history_ringing <lgl>, history_roaring <lgl>, history_vomiting <lgl>,\n#   late_wave_poor <lgl>, m_at_2k <lgl>, m_cond_lt_1k <lgl>, m_gt_1k <lgl>,\n#   m_m_gt_2k <lgl>, m_m_sn <lgl>, m_m_sn_gt_1k <lgl>, m_m_sn_gt_2k <lgl>, ...\n```\n:::\n:::\n\n\nRename the last two columns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(data)[70:71] <- c(\"id\", \"class\")\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 71\n   age_gt_60 air      airBoneGap ar_c     ar_u   bone  boneAbnormal bser \n   <lgl>     <chr>    <lgl>      <chr>    <chr>  <chr> <lgl>        <chr>\n 1 FALSE     mild     FALSE      normal   normal <NA>  TRUE         <NA> \n 2 FALSE     moderate FALSE      normal   normal <NA>  TRUE         <NA> \n 3 TRUE      mild     TRUE       <NA>     absent mild  TRUE         <NA> \n 4 TRUE      mild     TRUE       <NA>     absent mild  FALSE        <NA> \n 5 TRUE      mild     FALSE      normal   normal mild  TRUE         <NA> \n 6 TRUE      mild     FALSE      normal   normal mild  TRUE         <NA> \n 7 FALSE     mild     FALSE      normal   normal mild  TRUE         <NA> \n 8 FALSE     mild     FALSE      normal   normal mild  TRUE         <NA> \n 9 FALSE     severe   FALSE      <NA>     <NA>   <NA>  TRUE         <NA> \n10 TRUE      mild     FALSE      elevated absent mild  TRUE         <NA> \n# i 190 more rows\n# i 63 more variables: history_buzzing <lgl>, history_dizziness <lgl>,\n#   history_fluctuating <lgl>, history_fullness <lgl>, history_heredity <lgl>,\n#   history_nausea <lgl>, history_noise <lgl>, history_recruitment <lgl>,\n#   history_ringing <lgl>, history_roaring <lgl>, history_vomiting <lgl>,\n#   late_wave_poor <lgl>, m_at_2k <lgl>, m_cond_lt_1k <lgl>, m_gt_1k <lgl>,\n#   m_m_gt_2k <lgl>, m_m_sn <lgl>, m_m_sn_gt_1k <lgl>, m_m_sn_gt_2k <lgl>, ...\n```\n:::\n:::\n\n\nDONE!\n\n## Dealing with unknown values\n\nRemove observations or columns with many NAs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\nmissing.value.rows <- data |>\n  filter(!complete.cases(data))\nmissing.value.rows\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 196 x 71\n   age_gt_60 air      airBoneGap ar_c     ar_u   bone  boneAbnormal bser \n   <lgl>     <chr>    <lgl>      <chr>    <chr>  <chr> <lgl>        <chr>\n 1 FALSE     mild     FALSE      normal   normal <NA>  TRUE         <NA> \n 2 FALSE     moderate FALSE      normal   normal <NA>  TRUE         <NA> \n 3 TRUE      mild     TRUE       <NA>     absent mild  TRUE         <NA> \n 4 TRUE      mild     TRUE       <NA>     absent mild  FALSE        <NA> \n 5 TRUE      mild     FALSE      normal   normal mild  TRUE         <NA> \n 6 TRUE      mild     FALSE      normal   normal mild  TRUE         <NA> \n 7 FALSE     mild     FALSE      normal   normal mild  TRUE         <NA> \n 8 FALSE     mild     FALSE      normal   normal mild  TRUE         <NA> \n 9 FALSE     severe   FALSE      <NA>     <NA>   <NA>  TRUE         <NA> \n10 TRUE      mild     FALSE      elevated absent mild  TRUE         <NA> \n# i 186 more rows\n# i 63 more variables: history_buzzing <lgl>, history_dizziness <lgl>,\n#   history_fluctuating <lgl>, history_fullness <lgl>, history_heredity <lgl>,\n#   history_nausea <lgl>, history_noise <lgl>, history_recruitment <lgl>,\n#   history_ringing <lgl>, history_roaring <lgl>, history_vomiting <lgl>,\n#   late_wave_poor <lgl>, m_at_2k <lgl>, m_cond_lt_1k <lgl>, m_gt_1k <lgl>,\n#   m_m_gt_2k <lgl>, m_m_sn <lgl>, m_m_sn_gt_1k <lgl>, m_m_sn_gt_2k <lgl>, ...\n```\n:::\n:::\n\n\n196 out of 200 rows contain an NA!\n\nHow many NAs in each row? Apply a temporary function to the rows (\"1\", if to columns use \"2\") of data. This function counts the number of NAs in a row. If is.na(x) is TRUE (equivalent to 1), the sum of the booleans is then the count.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data %>%\n  mutate(na_count = rowSums(is.na(data)))\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 72\n   age_gt_60 air      airBoneGap ar_c     ar_u   bone  boneAbnormal bser \n   <lgl>     <chr>    <lgl>      <chr>    <chr>  <chr> <lgl>        <chr>\n 1 FALSE     mild     FALSE      normal   normal <NA>  TRUE         <NA> \n 2 FALSE     moderate FALSE      normal   normal <NA>  TRUE         <NA> \n 3 TRUE      mild     TRUE       <NA>     absent mild  TRUE         <NA> \n 4 TRUE      mild     TRUE       <NA>     absent mild  FALSE        <NA> \n 5 TRUE      mild     FALSE      normal   normal mild  TRUE         <NA> \n 6 TRUE      mild     FALSE      normal   normal mild  TRUE         <NA> \n 7 FALSE     mild     FALSE      normal   normal mild  TRUE         <NA> \n 8 FALSE     mild     FALSE      normal   normal mild  TRUE         <NA> \n 9 FALSE     severe   FALSE      <NA>     <NA>   <NA>  TRUE         <NA> \n10 TRUE      mild     FALSE      elevated absent mild  TRUE         <NA> \n# i 190 more rows\n# i 64 more variables: history_buzzing <lgl>, history_dizziness <lgl>,\n#   history_fluctuating <lgl>, history_fullness <lgl>, history_heredity <lgl>,\n#   history_nausea <lgl>, history_noise <lgl>, history_recruitment <lgl>,\n#   history_ringing <lgl>, history_roaring <lgl>, history_vomiting <lgl>,\n#   late_wave_poor <lgl>, m_at_2k <lgl>, m_cond_lt_1k <lgl>, m_gt_1k <lgl>,\n#   m_m_gt_2k <lgl>, m_m_sn <lgl>, m_m_sn_gt_1k <lgl>, m_m_sn_gt_2k <lgl>, ...\n```\n:::\n:::\n\n\nMaximum missing values in a row is 7, out of 69 dimensions, so they are not too bad.\n\nExamine columns: how many NAs in each variable/column?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  summarize(across(everything(), ~sum(is.na(.)), .names = \"na_{.col}\")) %>%\n  pivot_longer(everything(), names_to = \"column_name\", values_to = \"na_count\") %>%\n  arrange(na_count)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 72 x 2\n   column_name            na_count\n   <chr>                     <int>\n 1 na_age_gt_60                  0\n 2 na_air                        0\n 3 na_airBoneGap                 0\n 4 na_boneAbnormal               0\n 5 na_history_buzzing            0\n 6 na_history_dizziness          0\n 7 na_history_fluctuating        0\n 8 na_history_fullness           0\n 9 na_history_heredity           0\n10 na_history_nausea             0\n# i 62 more rows\n```\n:::\n:::\n\n\n`bser` variable has 196 NAs. If this variable is considered not useful, given some domain knowledge, we can remove it from the data. From View, I can see bser is the 8th column:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.bser.removed <- data %>%\n  select(-8) %>%\n  summarise(across(everything(), ~sum(is.na(.)), .names = \"na_{.col}\"))\ndata.bser.removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 71\n  na_age_gt_60 na_air na_airBoneGap na_ar_c na_ar_u na_bone na_boneAbnormal\n         <int>  <int>         <int>   <int>   <int>   <int>           <int>\n1            0      0             0       4       3      75               0\n# i 64 more variables: na_history_buzzing <int>, na_history_dizziness <int>,\n#   na_history_fluctuating <int>, na_history_fullness <int>,\n#   na_history_heredity <int>, na_history_nausea <int>, na_history_noise <int>,\n#   na_history_recruitment <int>, na_history_ringing <int>,\n#   na_history_roaring <int>, na_history_vomiting <int>,\n#   na_late_wave_poor <int>, na_m_at_2k <int>, na_m_cond_lt_1k <int>,\n#   na_m_gt_1k <int>, na_m_m_gt_2k <int>, na_m_m_sn <int>, ...\n```\n:::\n:::\n\n\n`matches` function can also help you find the index of a `colname` given its name:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data %>%\n  select(-matches(\"bser\"))\n```\n:::\n\n\n### Mistaken characters\n\nBecause R decides the data type based on what is given, sometimes, R's decision may not be what you meant. In the example below, because of a missing value `?`, R makes all other values in a vector 'character'. Parse_integer can be used to fix this problem.Â \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmistaken <- c(2, 3, 4, \"?\")\nclass(mistaken)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"character\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed <- parse_integer(mistaken, na = '?')\nfixed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2  3  4 NA\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(fixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"integer\"\n```\n:::\n:::\n\n\n### Filling unknowns with most frequent values\n\nAlways take cautions when you modify the original data.Â \n\n!!!Modifications must be well documented (so others can repeat your analysis) and well justified!!!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"DMwR2\")\nlibrary(DMwR2)\ndata(algae, package = \"DMwR2\")\nalgae[48,]\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 18\n  season size  speed  mxPH  mnO2    Cl   NO3   NH4  oPO4   PO4  Chla    a1    a2\n  <fct>  <fct> <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 winter small low      NA  12.6     9  0.23    10     5     6   1.1  35.5     0\n# i 5 more variables: a3 <dbl>, a4 <dbl>, a5 <dbl>, a6 <dbl>, a7 <dbl>\n```\n:::\n:::\n\n\n`mxPH` is unknown. Shall we fill in with mean, median or something else?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot a QQ plot of mxPH\ninstall.packages(\"car\")\nlibrary(car)\nggplot(algae, aes(sample = mxPH)) +\n  geom_qq_band() +\n  stat_qq_point() +\n    stat_qq_line(color = \"red\", method = \"identity\", intercept = -2, slope = 1) +  \n  ggtitle(\"Normal QQ plot of mxPH\") \n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'car'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    some\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    recode\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in stat_qq_line(color = \"red\", method = \"identity\", intercept = -2, :\nIgnoring unknown parameters: `method`, `intercept`, and `slope`\n```\n:::\n\n::: {.cell-output-display}\n![](rExercise2_files/figure-pdf/unnamed-chunk-47-1.pdf)\n:::\n:::\n\n\nThe straight line fits the data pretty well so `mxPH` is normal, use mean to fill the unknown.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgae <- algae |>\n  mutate(mxPH = ifelse(row_number() == 48, mean(mxPH, na.rm = TRUE), mxPH))\nalgae\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 18\n   season size  speed   mxPH  mnO2    Cl    NO3   NH4  oPO4   PO4  Chla    a1\n   <fct>  <fct> <fct>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 winter small medium  8      9.8  60.8  6.24  578   105   170   50      0  \n 2 spring small medium  8.35   8    57.8  1.29  370   429.  559.   1.3    1.4\n 3 autumn small medium  8.1   11.4  40.0  5.33  347.  126.  187.  15.6    3.3\n 4 spring small medium  8.07   4.8  77.4  2.30   98.2  61.2 139.   1.4    3.1\n 5 autumn small medium  8.06   9    55.4 10.4   234.   58.2  97.6 10.5    9.2\n 6 winter small high    8.25  13.1  65.8  9.25  430    18.2  56.7 28.4   15.1\n 7 summer small high    8.15  10.3  73.2  1.54  110    61.2 112.   3.2    2.4\n 8 autumn small high    8.05  10.6  59.1  4.99  206.   44.7  77.4  6.9   18.2\n 9 winter small medium  8.7    3.4  22.0  0.886 103.   36.3  71    5.54  25.4\n10 winter small high    7.93   9.9   8    1.39    5.8  27.2  46.6  0.8   17  \n# i 190 more rows\n# i 6 more variables: a2 <dbl>, a3 <dbl>, a4 <dbl>, a5 <dbl>, a6 <dbl>,\n#   a7 <dbl>\n```\n:::\n:::\n\n\nWhat about attribute `Chla`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(algae, aes(sample = Chla)) +\n  geom_qq_band() +\n  stat_qq_point() +\n    stat_qq_line(color = \"red\", method = \"identity\", intercept = -2, slope = 1) +  \n  ggtitle(\"Normal QQ plot of Chla\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in stat_qq_line(color = \"red\", method = \"identity\", intercept = -2, :\nIgnoring unknown parameters: `method`, `intercept`, and `slope`\n```\n:::\n\n::: {.cell-output-display}\n![](rExercise2_files/figure-pdf/unnamed-chunk-49-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian(algae$Chla, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.475\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(algae$Chla, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 13.9712\n```\n:::\n:::\n\n\nObviously, the mean is not a representative value for `Chla`. For this we will use median to fill all missing values in this attribute, instead of doing it one value at a time:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgae <- algae |>\n  mutate(Chla = if_else(is.na(Chla), median(Chla, na.rm = TRUE), Chla))\n```\n:::\n\n\n### Filling unknowns using linear regression\n\nThis method is used when two variables are highly correlated. One value of variable A can be used to predict the value for variable B using the linear regression model.\n\nFirst let's see what variables in algae are highly correlated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgae_numeric <- algae[, 4:18] %>%\n  drop_na()  # Removes rows with NA values\n\ncor_matrix <- algae_numeric |> correlate() |> plot()\n```\n\n::: {.cell-output-display}\n![](rExercise2_files/figure-pdf/unnamed-chunk-53-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ncor_matrix\n```\n\n::: {.cell-output-display}\n![](rExercise2_files/figure-pdf/unnamed-chunk-53-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# study only the numerical variables (4-18) and use only the complete observations -- obs with NAs are not used. symnum() makes the correlation matrix more readable\n```\n:::\n\n\nWe can see from the matrix, `PO4` and `oPO4` are correct with a confidence level of 90%.\n\nNext, we find the linear model between `PO4` and `oPO4`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgae <- algae %>%\n  filter(rowSums(is.na(.)) / ncol(.) < 0.2)#this is a method provided that selects the observations with 20% or move values as NAs. \n\nm = lm(PO4 ~ oPO4, data = algae)\nlm(formula = PO4 ~ oPO4, data = algae)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = PO4 ~ oPO4, data = algae)\n\nCoefficients:\n(Intercept)         oPO4  \n     42.897        1.293  \n```\n:::\n:::\n\n\nCheck the model, is it good? See <http://r-statistics.co/Linear-Regression.html>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm |> \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = PO4 ~ oPO4, data = algae)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-110.12  -36.34  -12.68   23.26  216.98 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   42.897      4.808   8.922 3.34e-16 ***\noPO4           1.293      0.041  31.535  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.37 on 195 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.8361,\tAdjusted R-squared:  0.8352 \nF-statistic: 994.5 on 1 and 195 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nor...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm |> \n  summary() |> \n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    42.9     4.81        8.92 3.34e-16\n2 oPO4            1.29    0.0410     31.5  1.68e-78\n```\n:::\n\n```{.r .cell-code}\n#tidy is from the tidymodels metapackage. This creates a more readable output for linear regressions\n```\n:::\n\n\nIf a good model, coefficients should all be significant (reject Ho coefficience is 0), Adjusted R-squared close to 1 (0.8 is very good).\n\nF-statistics p-value should be less than the significant level (typically 0.05).\n\nWhile R-squared provides an estimate of the strength of the relationship between your model and the response variable, it does not provide a formal hypothesis test for this relationship.\n\nThe [F-test of overall significance](#0) determines whether this relationship is statistically significant.\n\nThis model is good. We can also assess the fitness of the model with fitted line plot (should show the good fit), residual plot (should show residual being random).\n\nThis `lm` is `PO4 = 1.293*oPO4 + 42.897`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgae$PO4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 170.000 558.750 187.057 138.700  97.580  56.667 111.750  77.434  71.000\n [10]  46.600  20.750  19.000  17.000  15.000  61.600  98.250  50.000  57.833\n [19]  61.500 771.600 586.000  18.000  40.000  27.500  11.500  44.136  13.600\n [28]      NA  45.000  19.000 142.000 304.000 130.750  47.000  23.000  84.460\n [37]   3.000   3.000 253.250 255.280 296.000 175.046 344.600 326.857  40.667\n [46]  43.500  39.000   6.000 121.000  20.812  49.333  22.900  11.800  11.818\n [55]   6.500   1.000   4.000   6.000  11.000   6.000  14.000   6.667   6.750\n [64]   7.200   6.000  10.750   2.500 351.600 313.600 279.066 152.333  58.623\n [73] 249.250 233.500 215.500 102.333 105.727 111.375 108.000  56.667  60.000\n [82] 104.000  69.930 214.000 254.600 169.001 607.167 624.733 303.333 391.750\n [91] 265.250 232.833 244.000 218.000 138.500 239.000 235.667 205.875 211.667\n[100] 186.500 154.125 183.667 292.625 285.714 201.778 275.143 124.200 141.833\n[109] 132.546  17.333  26.000  16.662 102.571  86.997  10.111  18.293  13.200\n[118] 432.909 320.400 287.000 262.727 222.286 122.000 127.222  89.625 284.000\n[127] 277.333 177.625  72.900  82.444  66.750 173.750 317.000  84.000 213.000\n[136]  88.500 115.000  98.143 143.750 197.143  35.200  23.485 200.231 147.833\n[145] 276.000 123.333  75.207 116.200 188.667  72.696 116.200  34.000  76.333\n[154]  58.374 361.000 236.000 125.800  54.916  75.333 186.000 252.500 269.667\n[163]  46.438  85.000 171.272 232.900 146.452 246.667 219.909 272.222 388.167\n[172] 167.900 137.778 194.100 221.278  21.300  11.000  14.354   7.000   6.200\n[181]   7.654  16.000  56.091  52.875 228.364  85.400  87.125 101.455 127.000\n[190]  81.558  50.455 120.889  91.111  61.444  79.750  75.904 140.220 140.517\n```\n:::\n:::\n\n\nPO4 for observation 28 can then be filled with predicated value using the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgae <- algae %>%\n  mutate(PO4 = ifelse(row_number() == 28, 42.897 + 1.293 * oPO4, PO4))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres = resid(m)\n\noPO4_reduced <- algae %>%\n  filter(row_number() != 28) %>%\n  pull(oPO4)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = data.frame(oPO4 = m$model$oPO4, res = res), aes(x = oPO4, y = res)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"oPO4\",\n    y = \"residuals\",\n    title = \"Residual Plot\"\n  )\n```\n\n::: {.cell-output-display}\n![](rExercise2_files/figure-pdf/unnamed-chunk-60-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nIf there are more PO4 cells to fill, we can use `sapply()` to apply this transformation to a set of values\n\nCreate a simple function `fillPO4`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfillPO4 <- function(x) {\n  if_else(is.na(x), 42.897 + 1.293 * x, x)\n}\n#if x is not NA, return 42.897+1.293*x \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nalgae[is.na(algae$PO4), \"PO4\"] <- sapply(algae[is.na(algae$PO4), \"oPO4\"], fillPO4)\n```\n:::\n\n\nApply calls `fillPO4` function repeatedly, each time using one value in `algae[is.na(algae$PO4), \"oPO4\"]` as an argument.\n\nYou can perform a similar operation on subsets of observations. For example, if `PO4` is missing for data collected in **summer** and season affects `PO4` values, you may consider to use a linear model constructed with **summer** data only to obtain a more accurate prediction of `PO4` from `oPO4`.\n\n### Filling unknowns by exploring similarities among cases\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(algae, package=\"DMwR2\")\nalgae <- algae[-manyNAs(algae), ] \n```\n:::\n\n\n`DM2R2` provides a method call `knnImputation()`. This method use the Euclidean distance to find the ten most similar cases of any water sample with some unknown value in a variable, and then use their values to fill in the unknown.\n\nWe can simply calculate the median of the values of the ten nearest neighbors to fill in the gaps. In case of unknown nominal variables (which do not occur in our algae data set), we would use the most frequent value (the mode) among the neighbors. The second method uses a weighted average of the values of the neighbors.\n\nThe weights decrease as the distance to the case of the neighbors increases.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgae <- knnImputation(algae, k = 10) #use the weighted average of k most similar samples\n\n\ndata(algae, package=\"DMwR2\") #get data again so there are unknown values\nalgae <- algae[-manyNAs(algae), ] \nalgae <- knnImputation(algae, k = 10, meth=\"median\") #use the median of k most similar samples\n```\n:::\n\n\nTo see what is in `knnImputation()` (You are not required to read and understand the code):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetAnywhere(knnImputation())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA single object matching 'knnImputation' was found\nIt was found in the following places\n  package:DMwR2\n  namespace:DMwR2\nwith value\n\nfunction (data, k = 10, scale = TRUE, meth = \"weighAvg\", distData = NULL) \n{\n    n <- nrow(data)\n    if (!is.null(distData)) {\n        distInit <- n + 1\n        data <- rbind(data, distData)\n    }\n    else distInit <- 1\n    N <- nrow(data)\n    ncol <- ncol(data)\n    contAttrs <- which(vapply(data, dplyr::type_sum, character(1)) %in% \n        c(\"dbl\", \"int\"))\n    nomAttrs <- setdiff(seq.int(ncol), contAttrs)\n    hasNom <- length(nomAttrs)\n    dm <- data\n    if (scale) \n        dm[, contAttrs] <- scale(dm[, contAttrs])\n    if (hasNom) \n        for (i in nomAttrs) dm[[i]] <- as.integer(dm[[i]])\n    dm <- as.matrix(dm)\n    nas <- which(!complete.cases(dm))\n    if (!is.null(distData)) \n        tgt.nas <- nas[nas <= n]\n    else tgt.nas <- nas\n    if (length(tgt.nas) == 0) \n        warning(\"No case has missing values. Stopping as there is nothing to do.\")\n    xcomplete <- dm[setdiff(distInit:N, nas), ]\n    if (nrow(xcomplete) < k) \n        stop(\"Not sufficient complete cases for computing neighbors.\")\n    for (i in tgt.nas) {\n        tgtAs <- which(is.na(dm[i, ]))\n        dist <- scale(xcomplete, dm[i, ], FALSE)\n        xnom <- setdiff(nomAttrs, tgtAs)\n        if (length(xnom)) \n            dist[, xnom] <- ifelse(dist[, xnom] > 0, 1, dist[, \n                xnom])\n        dist <- dist[, -tgtAs]\n        dist <- sqrt(drop(dist^2 %*% rep(1, ncol(dist))))\n        ks <- order(dist)[seq(k)]\n        for (j in tgtAs) if (meth == \"median\") \n            data[i, j] <- centralValue(data[setdiff(distInit:N, \n                nas)[ks], j])\n        else data[i, j] <- centralValue(data[setdiff(distInit:N, \n            nas)[ks], j], exp(-dist[ks]))\n    }\n    data[1:n, ]\n}\n<bytecode: 0x11f315f40>\n<environment: namespace:DMwR2>\n```\n:::\n:::\n\n\n# Scaling and normalization\n\nNormalize value `x`: $y = (x-mean)/standard_deviation(x)$ using `scale()`\n\nNormalize values in penguins dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(palmerpenguins)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'palmerpenguins'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:modeldata':\n\n    penguins\n```\n:::\n\n```{.r .cell-code}\ndata(penguins)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# select only numeric columns\npenguins_numeric <- select(penguins, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)\n\n# normalize numeric columns\npenguins_norm <- scale(penguins_numeric)\n\n# convert back to data frame and add species column\npeng.norm <- cbind(as.data.frame(penguins_norm), species = penguins$species)\n\n# because scale() takes numeric matrix as input, we first remove Species column, then use cbind() to add the column back after normalization.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(peng.norm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n bill_length_mm     bill_depth_mm      flipper_length_mm  body_mass_g     \n Min.   :-2.16535   Min.   :-2.05144   Min.   :-2.0563   Min.   :-1.8726  \n 1st Qu.:-0.86031   1st Qu.:-0.78548   1st Qu.:-0.7762   1st Qu.:-0.8127  \n Median : 0.09672   Median : 0.07537   Median :-0.2784   Median :-0.1892  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.83854   3rd Qu.: 0.78430   3rd Qu.: 0.8594   3rd Qu.: 0.6836  \n Max.   : 2.87166   Max.   : 2.20217   Max.   : 2.1395   Max.   : 2.6164  \n NA's   :2          NA's   :2          NA's   :2         NA's   :2        \n      species   \n Adelie   :152  \n Chinstrap: 68  \n Gentoo   :124  \n                \n                \n                \n                \n```\n:::\n:::\n\n\n`scale()` can also take an argument for center and an argument of scale to normalize data in some other ways, for example, $y = (x-min)/(max-min)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax <- apply(select(penguins, -species), 2, max, na.rm=TRUE)\nmin <- apply(select(penguins, -species), 2, min, na.rm=TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmax\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           island    bill_length_mm     bill_depth_mm flipper_length_mm \n      \"Torgersen\"            \"59.6\"            \"21.5\"             \"231\" \n      body_mass_g               sex              year \n           \"6300\"            \"male\"            \"2009\" \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           island    bill_length_mm     bill_depth_mm flipper_length_mm \n         \"Biscoe\"            \"32.1\"            \"13.1\"             \"172\" \n      body_mass_g               sex              year \n           \"2700\"          \"female\"            \"2007\" \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# min-max normalization\npenguin_scaled <- as.data.frame(lapply(penguins_numeric, function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))\n\npenguin_scaled <- cbind(penguins_norm, species = penguins$species)\n\nsummary(penguin_scaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n bill_length_mm     bill_depth_mm      flipper_length_mm  body_mass_g     \n Min.   :-2.16535   Min.   :-2.05144   Min.   :-2.0563   Min.   :-1.8726  \n 1st Qu.:-0.86031   1st Qu.:-0.78548   1st Qu.:-0.7762   1st Qu.:-0.8127  \n Median : 0.09672   Median : 0.07537   Median :-0.2784   Median :-0.1892  \n Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.83854   3rd Qu.: 0.78430   3rd Qu.: 0.8594   3rd Qu.: 0.6836  \n Max.   : 2.87166   Max.   : 2.20217   Max.   : 2.1395   Max.   : 2.6164  \n NA's   :2          NA's   :2          NA's   :2         NA's   :2        \n    species     \n Min.   :1.000  \n 1st Qu.:1.000  \n Median :2.000  \n Mean   :1.919  \n 3rd Qu.:3.000  \n Max.   :3.000  \n                \n```\n:::\n:::\n\n\n## Discretizing variables (binning)\n\nThe process of transferring continuous functions, models, variables, and equations into discrete counterparts\n\nUse `dlookr`'s `binning(type = \"equal\")` for equal-length cuts (bins)\n\nUse `Hmisc`'s `cut2()` for equal-depth cuts\n\nBoston Housing data as an example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Boston, package=\"MASS\")\nsummary(Boston$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.90   45.02   77.50   68.57   94.08  100.00 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nBoston$newAge <- dlookr::binning(Boston$age, 5, type = \"equal\") #create 5 bins and add new column newAge to Boston\nsummary(Boston$newAge)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         levels freq       rate\n1   [2.9,22.32]   45 0.08893281\n2 (22.32,41.74]   71 0.14031621\n3 (41.74,61.16]   70 0.13833992\n4 (61.16,80.58]   81 0.16007905\n5   (80.58,100]  239 0.47233202\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nBoston$newAge <- dlookr::binning(Boston$age, nbins = 5, labels = c(\"very-young\", \"young\", \"mid\", \"older\", \"very-old\"), type = \"equal\") #add labels\n\nsummary(Boston$newAge)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      levels freq       rate\n1 very-young   45 0.08893281\n2      young   71 0.14031621\n3        mid   70 0.13833992\n4      older   81 0.16007905\n5   very-old  239 0.47233202\n```\n:::\n:::\n\n\n### Equal-depth\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"Hmisc\")\nlibrary(Hmisc)\nBoston$newAge <- cut2(Boston$age, g = 5) #create 5 equal-depth bins and add new column newAge to Boston\n\ntable(Boston$newAge)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Hmisc'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:parsnip':\n\n    translate\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dlookr':\n\n    describe\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[ 2.9, 38.1) [38.1, 66.1) [66.1, 86.1) [86.1, 95.7) [95.7,100.0] \n         102          101          101          101          101 \n```\n:::\n:::\n\n\n### Assign labels\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBoston$newAge <- factor(cut2(Boston$age, g = 5), labels = c(\"very-young\", \"young\", \"mid\", \"older\", \"very-old\"))\n\ntable(Boston$newAge)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nvery-young      young        mid      older   very-old \n       102        101        101        101        101 \n```\n:::\n:::\n\n\nPlot an equal-width histogram of width 10:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(Boston$age, breaks = seq(0, 101,by = 10)) #seq() gives the function for breaks. The age ranges from 0 â€“ 101.\n```\n\n::: {.cell-output-display}\n![](rExercise2_files/figure-pdf/unnamed-chunk-80-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nor, use `ggplot2`!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nBoston |>\n  ggplot(aes(x = age)) +\n  geom_histogram(binwidth = 10)\n```\n\n::: {.cell-output-display}\n![](rExercise2_files/figure-pdf/unnamed-chunk-81-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### `dlookr` also has great binning functions you can try!\n\n<https://choonghyunryu.github.io/dlookr/reference/binning.html>\n\n## Decimal scaling\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- c(10, 20, 30, 50, 100)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(nDigits = nchar(max(abs(data)))) #nchar counts the number of characters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(decimalScale = data / (10^nDigits))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01 0.02 0.03 0.05 0.10\n```\n:::\n:::\n\n\n### Smoothing by bin mean\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage = c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30)\n\n# Separate data into bins of depth 3\n(bins = matrix(age, nrow = length(age) / 5, byrow = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   13   15   16   16   19\n[2,]   20   20   21   22   22\n[3,]   25   25   25   25   30\n```\n:::\n:::\n\n\nFind the average of each bin:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(bin_means = apply(bins, 1, FUN = mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15.8 21.0 26.0\n```\n:::\n:::\n\n\nReplace values with their bin mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 1:nrow(bins)) {\n   bins[i,] = bin_means[i]\n }\nbins\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,] 15.8 15.8 15.8 15.8 15.8\n[2,] 21.0 21.0 21.0 21.0 21.0\n[3,] 26.0 26.0 26.0 26.0 26.0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(age_bin_mean_smoothed = round(as.vector(t(bins)), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 15.8 15.8 15.8 15.8 15.8 21.0 21.0 21.0 21.0 21.0 26.0 26.0 26.0 26.0 26.0\n```\n:::\n:::\n\n\n# Variable correlations and dimensionality reduction\n\n## Chi-squared test\n\nH0: (Prisoner's race)(Victim's race) are independent.\n\ndata (contingency table):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nracetable = rbind(c(151,9), c(63,103))\ntest1 = chisq.test(racetable, correct=F)\ntest1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  racetable\nX-squared = 115.01, df = 1, p-value < 2.2e-16\n```\n:::\n:::\n\n\np-value is less than 0.05: chance to get X-squared value of 115.01 assuming H0 is true is very slim (close to 0), so reject H0.\n\n## Loglinear model\n\nExtending chi-squared to more than 2 categorical variables.\n\nLoglinear models model cell counts in contingency tables.\n\nAdapted from <https://data.library.virginia.edu/an-introduction-to-loglinear-models/>:\n\nData from Agresti (1996, Table 6.3) modified. It summarizes responses from a survey that asked high school seniors in a particular city whether they had ever used alcohol, cigarettes, or marijuana. A categorical variable age is added.\n\nWe're interested in how the cell counts in the contingency table depend on the levels of the categorical variables.\n\nGet the data in. We will be analyzing cells in contingency tables, so use a multi-dimensional array to hold the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseniors <- array(data = c(911, 44, 538, 456, 3, 2, 43, 279, 911, 44, 538, 456, 3, 2, 43, 279), \n                  dim = c(2, 2, 2, 2),\n                  dimnames = list(\"cigarette\" = c(\"yes\", \"no\"),\n                                  \"marijuana\" = c(\"yes\", \"no\"),\n                                  \"alcohol\" = c(\"yes\", \"no\"), \n                                  \"age\" =c(\"younger\", \"older\")))\n```\n:::\n\n\nObserve how data is saved in the 2x2x2x2 array:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseniors\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n, , alcohol = yes, age = younger\n\n         marijuana\ncigarette yes  no\n      yes 911 538\n      no   44 456\n\n, , alcohol = no, age = younger\n\n         marijuana\ncigarette yes  no\n      yes   3  43\n      no    2 279\n\n, , alcohol = yes, age = older\n\n         marijuana\ncigarette yes  no\n      yes 911 538\n      no   44 456\n\n, , alcohol = no, age = older\n\n         marijuana\ncigarette yes  no\n      yes   3  43\n      no    2 279\n```\n:::\n:::\n\n\nNext, do loglinear modeling using the glm function (generalized linear models).\n\nWe need to convert the array to a table then to a data frame.\n\nCalling `as.data.frame` on a table object in R returns a data frame with a column for cell frequencies where each row represents a unique combination of variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseniors.tb <- as.table(seniors)\nseniors.tb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n, , alcohol = yes, age = younger\n\n         marijuana\ncigarette yes  no\n      yes 911 538\n      no   44 456\n\n, , alcohol = no, age = younger\n\n         marijuana\ncigarette yes  no\n      yes   3  43\n      no    2 279\n\n, , alcohol = yes, age = older\n\n         marijuana\ncigarette yes  no\n      yes 911 538\n      no   44 456\n\n, , alcohol = no, age = older\n\n         marijuana\ncigarette yes  no\n      yes   3  43\n      no    2 279\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nseniors.df <- as.data.frame(seniors.tb)\nseniors.df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   cigarette marijuana alcohol     age Freq\n1        yes       yes     yes younger  911\n2         no       yes     yes younger   44\n3        yes        no     yes younger  538\n4         no        no     yes younger  456\n5        yes       yes      no younger    3\n6         no       yes      no younger    2\n7        yes        no      no younger   43\n8         no        no      no younger  279\n9        yes       yes     yes   older  911\n10        no       yes     yes   older   44\n11       yes        no     yes   older  538\n12        no        no     yes   older  456\n13       yes       yes      no   older    3\n14        no       yes      no   older    2\n15       yes        no      no   older   43\n16        no        no      no   older  279\n```\n:::\n:::\n\n\nNext, we model Freq (this is the count in the contingency table) as a function of the three variables using the glm function. Set `family = poisson` because we are assuming independent counts.Â \n\nPoisson distribution: discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event.\n\nOur H0 is the four variables are independent of one another.\n\nWe will test the saturated model first (including all variables and all two-way and three-way interactions), because it will show the significance for all variables and their interactions\n\nUse `*` to connect all variables to get a saturated model, which will fit the data perfectly. Then we will remove effects that are not significant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.S4 <- glm(Freq ~ (cigarette * marijuana * alcohol * age), data = seniors.df, family=poisson)\nsummary(mod.S4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Freq ~ (cigarette * marijuana * alcohol * age), \n    family = poisson, data = seniors.df)\n\nCoefficients:\n                                             Estimate Std. Error z value\n(Intercept)                                 6.815e+00  3.313e-02 205.682\ncigaretteno                                -3.030e+00  1.544e-01 -19.633\nmarijuanano                                -5.267e-01  5.437e-02  -9.686\nalcoholno                                  -5.716e+00  5.783e-01  -9.884\nageolder                                   -1.946e-15  4.685e-02   0.000\ncigaretteno:marijuanano                     2.865e+00  1.670e-01  17.159\ncigaretteno:alcoholno                       2.625e+00  9.258e-01   2.835\nmarijuanano:alcoholno                       3.189e+00  5.996e-01   5.319\ncigaretteno:ageolder                        7.281e-15  2.183e-01   0.000\nmarijuanano:ageolder                        1.351e-15  7.690e-02   0.000\nalcoholno:ageolder                          2.098e-15  8.178e-01   0.000\ncigaretteno:marijuanano:alcoholno          -5.895e-01  9.424e-01  -0.626\ncigaretteno:marijuanano:ageolder           -6.706e-15  2.361e-01   0.000\ncigaretteno:alcoholno:ageolder             -3.770e-15  1.309e+00   0.000\nmarijuanano:alcoholno:ageolder             -1.143e-15  8.480e-01   0.000\ncigaretteno:marijuanano:alcoholno:ageolder  3.239e-15  1.333e+00   0.000\n                                           Pr(>|z|)    \n(Intercept)                                 < 2e-16 ***\ncigaretteno                                 < 2e-16 ***\nmarijuanano                                 < 2e-16 ***\nalcoholno                                   < 2e-16 ***\nageolder                                    1.00000    \ncigaretteno:marijuanano                     < 2e-16 ***\ncigaretteno:alcoholno                       0.00458 ** \nmarijuanano:alcoholno                      1.04e-07 ***\ncigaretteno:ageolder                        1.00000    \nmarijuanano:ageolder                        1.00000    \nalcoholno:ageolder                          1.00000    \ncigaretteno:marijuanano:alcoholno           0.53160    \ncigaretteno:marijuanano:ageolder            1.00000    \ncigaretteno:alcoholno:ageolder              1.00000    \nmarijuanano:alcoholno:ageolder              1.00000    \ncigaretteno:marijuanano:alcoholno:ageolder  1.00000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5.7029e+03  on 15  degrees of freedom\nResidual deviance: 8.7041e-14  on  0  degrees of freedom\nAIC: 130.09\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n:::\n\n\n*Note*: \"Residual deviance\" indicates the fitness of the model to the data. A good fit would have residual deviance less than or close to its degree of freedom. That is the case for the saturated model, which is expected.\n\nThen look at \"Coefficients\" (these are the lamdas). Many of them are not significant (\\*, \\*\\*, \\*\\*\\* indicates significant lamdas)\n\nBy examining those insignificant effects, we see they all involve `age`.\n\nNow lets' remove age and re-generate a model with the remaining three variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.S3 <- glm(Freq ~ (cigarette * marijuana * alcohol), data = seniors.df, family = poisson)\nsummary(mod.S3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Freq ~ (cigarette * marijuana * alcohol), family = poisson, \n    data = seniors.df)\n\nCoefficients:\n                                  Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                        6.81454    0.02343 290.878  < 2e-16 ***\ncigaretteno                       -3.03035    0.10914 -27.765  < 2e-16 ***\nmarijuanano                       -0.52668    0.03845 -13.699  < 2e-16 ***\nalcoholno                         -5.71593    0.40892 -13.978  < 2e-16 ***\ncigaretteno:marijuanano            2.86499    0.11806  24.267  < 2e-16 ***\ncigaretteno:alcoholno              2.62489    0.65466   4.010 6.08e-05 ***\nmarijuanano:alcoholno              3.18927    0.42400   7.522 5.40e-14 ***\ncigaretteno:marijuanano:alcoholno -0.58951    0.66635  -0.885    0.376    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance:  5.7029e+03  on 15  degrees of freedom\nResidual deviance: -1.7897e-13  on  8  degrees of freedom\nAIC: 114.09\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n:::\n\n\nWe see the model fits well, and most effects are significant now.\n\nThe insignificant one is the 3-way interaction among the three factors\n\nFor data reduction, we are done -- we removed `age` variable. Because all cigarette, marijuana, and alcohol effects are significant, we can't remove any of these from the data set, even though the 3-way interaction is not significant.\n\nFor data modeling, we can remove the 3-way interaction by testing \"`Freq ~ (cigarette + marijuana + alcohol)^2`\" (`^2` tells glm to check only two way interactions).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.3 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, data = seniors.df, family = poisson)\nsummary(mod.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Freq ~ (cigarette + marijuana + alcohol)^2, family = poisson, \n    data = seniors.df)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              6.81387    0.02342 290.902   <2e-16 ***\ncigaretteno             -3.01575    0.10721 -28.130   <2e-16 ***\nmarijuanano             -0.52486    0.03838 -13.674   <2e-16 ***\nalcoholno               -5.52827    0.31976 -17.289   <2e-16 ***\ncigaretteno:marijuanano  2.84789    0.11585  24.582   <2e-16 ***\ncigaretteno:alcoholno    2.05453    0.12308  16.692   <2e-16 ***\nmarijuanano:alcoholno    2.98601    0.32858   9.088   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5702.92195  on 15  degrees of freedom\nResidual deviance:    0.74797  on  9  degrees of freedom\nAIC: 112.83\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nNow compare the fitted and observed values and see how well they match up:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(mod.3$data, fitted(mod.3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   cigarette marijuana alcohol     age Freq fitted(mod.3)\n1        yes       yes     yes younger  911     910.38317\n2         no       yes     yes younger   44      44.61683\n3        yes        no     yes younger  538     538.61683\n4         no        no     yes younger  456     455.38317\n5        yes       yes      no younger    3       3.61683\n6         no       yes      no younger    2       1.38317\n7        yes        no      no younger   43      42.38317\n8         no        no      no younger  279     279.61683\n9        yes       yes     yes   older  911     910.38317\n10        no       yes     yes   older   44      44.61683\n11       yes        no     yes   older  538     538.61683\n12        no        no     yes   older  456     455.38317\n13       yes       yes      no   older    3       3.61683\n14        no       yes      no   older    2       1.38317\n15       yes        no      no   older   43      42.38317\n16        no        no      no   older  279     279.61683\n```\n:::\n:::\n\n\nThey fit well!\n\n## Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr) # data manipulation\npenguins_numeric |> \n  drop_na() |>\n  correlate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12 x 3\n   var1              var2              coef_corr\n   <fct>             <fct>                 <dbl>\n 1 bill_depth_mm     bill_length_mm       -0.235\n 2 flipper_length_mm bill_length_mm        0.656\n 3 body_mass_g       bill_length_mm        0.595\n 4 bill_length_mm    bill_depth_mm        -0.235\n 5 flipper_length_mm bill_depth_mm        -0.584\n 6 body_mass_g       bill_depth_mm        -0.472\n 7 bill_length_mm    flipper_length_mm     0.656\n 8 bill_depth_mm     flipper_length_mm    -0.584\n 9 body_mass_g       flipper_length_mm     0.871\n10 bill_length_mm    body_mass_g           0.595\n11 bill_depth_mm     body_mass_g          -0.472\n12 flipper_length_mm body_mass_g           0.871\n```\n:::\n:::\n\n\n`bill_length_mm` and `flipper_length_mm` are highly negatively correlated, `body_mass_g` and `flipper_length_mm` are strongly positively correlated as well.\n\n## Principal components analysis (PCA)\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca.data <- penguins |>\n  drop_na() |>\n  select(-species, -island, -sex) \n\npca <- princomp(pca.data)\nloadings(pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLoadings:\n                  Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nbill_length_mm            0.319  0.941  0.108       \nbill_depth_mm                    0.144 -0.984       \nflipper_length_mm         0.943 -0.304 -0.125       \nbody_mass_g        1.000                            \nyear                                           0.997\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nSS loadings       1.0    1.0    1.0    1.0    1.0\nProportion Var    0.2    0.2    0.2    0.2    0.2\nCumulative Var    0.2    0.4    0.6    0.8    1.0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(pca$scores) # pca result is a list, and the component scores are elements in the list\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Comp.1     Comp.2      Comp.3     Comp.4     Comp.5\n[1,] -457.3251 -13.376298  1.24790414  0.3764738 -0.5892777\n[2,] -407.2522  -9.205245 -0.03266674  1.0902167 -0.6935631\n[3,] -957.0447   8.128321 -2.49146655 -0.7208225 -1.3723988\n[4,] -757.1158   1.838910 -4.88056871 -2.0736684 -1.3477178\n[5,] -557.1773  -3.416994 -1.12926657 -2.6292970 -1.1451328\n[6,] -582.3093 -11.398030  0.80239704  1.1987261 -0.5976776\n```\n:::\n:::\n\n\nIf we are happy with capturing 75% of the original variance of the cases, we can reduce the original data to the first three components.\n\nComponent scores are computed based on the loading, for example:\n\n``` comp3 = 0.941*bill_length_mm + 0.144*``bill_depth_mm``- 0.309*flipper_length_mm ```\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_na <- penguins |> \n  drop_na()\n\npeng.reduced <- data.frame(pca$scores[,1:3], Species = penguins_na$species)\n\nhead(peng.reduced)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Comp.1     Comp.2      Comp.3 Species\n1 -457.3251 -13.376298  1.24790414  Adelie\n2 -407.2522  -9.205245 -0.03266674  Adelie\n3 -957.0447   8.128321 -2.49146655  Adelie\n4 -757.1158   1.838910 -4.88056871  Adelie\n5 -557.1773  -3.416994 -1.12926657  Adelie\n6 -582.3093 -11.398030  0.80239704  Adelie\n```\n:::\n:::\n\nNow we can use `peng.reduced` data frame for subsequent analyses:\n\nHaar Discrete Wavelet Transform, using the example shown in class\n\nOutput:\n\n|     |                                                                                                         |\n|------------|------------------------------------------------------------|\n| W:  | A list with elementÂ iÂ comprised of a matrix containing theÂ ith level wavelet coefficients (differences) |\n| V:Â  | A list with elementÂ iÂ comprised of a matrix containing theÂ ith level scaling coefficients (averages).   |\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"wavelets\")\nlibrary(wavelets)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'wavelets'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:car':\n\n    dwt\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(2, 2, 0, 2, 3, 5, 4, 4)\nwt <- dwt(x,filter=\"haar\", n.levels = 3) #with 8-element vector, 3 level is the max.\nwt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAn object of class \"dwt\"\nSlot \"W\":\n$W1\n         [,1]\n[1,] 0.000000\n[2,] 1.414214\n[3,] 1.414214\n[4,] 0.000000\n\n$W2\n     [,1]\n[1,]   -1\n[2,]    0\n\n$W3\n         [,1]\n[1,] 3.535534\n\n\nSlot \"V\":\n$V1\n         [,1]\n[1,] 2.828427\n[2,] 1.414214\n[3,] 5.656854\n[4,] 5.656854\n\n$V2\n     [,1]\n[1,]    3\n[2,]    8\n\n$V3\n         [,1]\n[1,] 7.778175\n\n\nSlot \"filter\":\nFilter Class: Daubechies\nName: HAAR\nLength: 2\nLevel: 1\nWavelet Coefficients: 7.0711e-01 -7.0711e-01\nScaling Coefficients: 7.0711e-01 7.0711e-01\n\n\nSlot \"level\":\n[1] 3\n\nSlot \"n.boundary\":\n[1] 0 0 0\n\nSlot \"boundary\":\n[1] \"periodic\"\n\nSlot \"series\":\n     [,1]\n[1,]    2\n[2,]    2\n[3,]    0\n[4,]    2\n[5,]    3\n[6,]    5\n[7,]    4\n[8,]    4\n\nSlot \"class.X\":\n[1] \"numeric\"\n\nSlot \"attr.X\":\nlist()\n\nSlot \"aligned\":\n[1] FALSE\n\nSlot \"coe\":\n[1] FALSE\n```\n:::\n:::\n\n\nWhy aren't the W and V vectors having the same values as shown in class?\n\nBecause in class, use simply taking the average and differences/2, in the default Haar, the default coefficients are sqrt(2)/2, see the section in bold above.\n\nReconstruct the original:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nidwt(wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 2 0 2 3 5 4 4\n```\n:::\n:::\n\n\nObtain transform results as shown in class, use a different filter:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxt = dwt(x, filter = wt.filter(c(0.5, -0.5)), n.levels = 3)\nxt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAn object of class \"dwt\"\nSlot \"W\":\n$W1\n     [,1]\n[1,]    0\n[2,]    1\n[3,]    1\n[4,]    0\n\n$W2\n     [,1]\n[1,] -0.5\n[2,]  0.0\n\n$W3\n     [,1]\n[1,] 1.25\n\n\nSlot \"V\":\n$V1\n     [,1]\n[1,]    2\n[2,]    1\n[3,]    4\n[4,]    4\n\n$V2\n     [,1]\n[1,]  1.5\n[2,]  4.0\n\n$V3\n     [,1]\n[1,] 2.75\n\n\nSlot \"filter\":\nFilter Class: none\nName: NONE\nLength: 2\nLevel: 1\nWavelet Coefficients: 5.0000e-01 -5.0000e-01\nScaling Coefficients: 5.0000e-01 5.0000e-01\n\n\nSlot \"level\":\n[1] 3\n\nSlot \"n.boundary\":\n[1] 0 0 0\n\nSlot \"boundary\":\n[1] \"periodic\"\n\nSlot \"series\":\n     [,1]\n[1,]    2\n[2,]    2\n[3,]    0\n[4,]    2\n[5,]    3\n[6,]    5\n[7,]    4\n[8,]    4\n\nSlot \"class.X\":\n[1] \"numeric\"\n\nSlot \"attr.X\":\nlist()\n\nSlot \"aligned\":\n[1] FALSE\n\nSlot \"coe\":\n[1] FALSE\n```\n:::\n:::\n\n\nReconstruct the original:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nidwt(xt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  0.3125  0.3125 -0.4375  0.5625  0.0000  1.0000  0.5000  0.5000\n```\n:::\n:::\n\n\n# Sampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nage <- c(25, 25, 25, 30, 33, 33, 35, 40, 45, 46, 52, 70)\n```\n:::\n\n\n## Simple random sampling, without replacement:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(age, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 45 30 35 25 25\n```\n:::\n:::\n\n\n## Simple random sampling, with replacement:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(age, 5, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 35 52 25 52 25\n```\n:::\n:::\n\n\n## Stratified sampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nset.seed(1) #make results the same each run\nsummary(algae)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    season       size       speed         mxPH            mnO2       \n autumn:40   large :44   high  :84   Min.   :5.600   Min.   : 1.500  \n spring:53   medium:84   low   :33   1st Qu.:7.705   1st Qu.: 7.825  \n summer:44   small :70   medium:81   Median :8.060   Median : 9.800  \n winter:61                           Mean   :8.019   Mean   : 9.135  \n                                     3rd Qu.:8.400   3rd Qu.:10.800  \n                                     Max.   :9.700   Max.   :13.400  \n       Cl               NO3              NH4                oPO4       \n Min.   :  0.222   Min.   : 0.050   Min.   :    5.00   Min.   :  1.00  \n 1st Qu.: 10.425   1st Qu.: 1.296   1st Qu.:   38.33   1st Qu.: 15.70  \n Median : 32.178   Median : 2.675   Median :  103.17   Median : 40.15  \n Mean   : 42.434   Mean   : 3.282   Mean   :  501.30   Mean   : 73.59  \n 3rd Qu.: 57.492   3rd Qu.: 4.446   3rd Qu.:  226.95   3rd Qu.: 99.33  \n Max.   :391.500   Max.   :45.650   Max.   :24064.00   Max.   :564.60  \n      PO4              Chla               a1               a2        \n Min.   :  1.00   Min.   :  0.200   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 41.38   1st Qu.:  2.000   1st Qu.: 1.525   1st Qu.: 0.000  \n Median :103.29   Median :  5.155   Median : 6.950   Median : 3.000  \n Mean   :137.88   Mean   : 13.355   Mean   :16.996   Mean   : 7.471  \n 3rd Qu.:213.75   3rd Qu.: 17.200   3rd Qu.:24.800   3rd Qu.:11.275  \n Max.   :771.60   Max.   :110.456   Max.   :89.800   Max.   :72.600  \n       a3               a4               a5               a6        \n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000  \n Median : 1.550   Median : 0.000   Median : 2.000   Median : 0.000  \n Mean   : 4.334   Mean   : 1.997   Mean   : 5.116   Mean   : 6.005  \n 3rd Qu.: 4.975   3rd Qu.: 2.400   3rd Qu.: 7.500   3rd Qu.: 6.975  \n Max.   :42.800   Max.   :44.600   Max.   :44.400   Max.   :77.600  \n       a7        \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 1.000  \n Mean   : 2.487  \n 3rd Qu.: 2.400  \n Max.   :31.600  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsample <-algae |> group_by(season) |> sample_frac(0.25)\nsummary(sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    season       size       speed         mxPH            mnO2       \n autumn:10   large :18   high  :16   Min.   :5.600   Min.   : 1.500  \n spring:13   medium:20   low   :13   1st Qu.:7.840   1st Qu.: 8.200  \n summer:11   small :11   medium:20   Median :8.100   Median : 9.700  \n winter:15                           Mean   :8.126   Mean   : 9.084  \n                                     3rd Qu.:8.500   3rd Qu.:10.800  \n                                     Max.   :9.700   Max.   :11.800  \n       Cl               NO3             NH4               oPO4       \n Min.   :  0.222   Min.   :0.130   Min.   :   5.00   Min.   :  1.00  \n 1st Qu.:  9.055   1st Qu.:0.921   1st Qu.:  43.75   1st Qu.: 21.08  \n Median : 28.333   Median :2.680   Median : 103.33   Median : 49.00  \n Mean   : 39.175   Mean   :2.794   Mean   : 213.60   Mean   : 78.49  \n 3rd Qu.: 51.113   3rd Qu.:4.053   3rd Qu.: 225.00   3rd Qu.:125.67  \n Max.   :173.750   Max.   :6.640   Max.   :1495.00   Max.   :267.75  \n      PO4              Chla               a1              a2       \n Min.   :  1.00   Min.   :  0.300   Min.   : 0.00   Min.   : 0.00  \n 1st Qu.: 69.93   1st Qu.:  2.500   1st Qu.: 1.20   1st Qu.: 1.50  \n Median :125.80   Median :  6.209   Median : 5.70   Median : 5.60  \n Mean   :142.05   Mean   : 14.848   Mean   :17.94   Mean   :10.24  \n 3rd Qu.:228.36   3rd Qu.: 20.829   3rd Qu.:18.10   3rd Qu.:14.50  \n Max.   :391.75   Max.   :110.456   Max.   :86.60   Max.   :72.60  \n       a3               a4               a5               a6       \n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   : 0.00  \n 1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.00  \n Median : 1.400   Median : 0.000   Median : 1.200   Median : 0.00  \n Mean   : 3.498   Mean   : 1.349   Mean   : 4.678   Mean   : 6.12  \n 3rd Qu.: 4.000   3rd Qu.: 1.800   3rd Qu.: 7.700   3rd Qu.: 5.10  \n Max.   :24.800   Max.   :13.400   Max.   :22.100   Max.   :52.50  \n       a7        \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 1.000  \n Mean   : 2.367  \n 3rd Qu.: 2.200  \n Max.   :31.200  \n```\n:::\n:::\n\n\n## Cluster sampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sampling)\nage <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)\ns <- kmeans(age, 3) #cluster on age to form 3 clusters\ns$cluster\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nageframe <- data.frame(age)\nageframe$condition <- s$cluster # add cluster label as condition\ncluster(ageframe, clustername = \"condition\", size = 2) # select 2 clusters out of the three\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in cluster(ageframe, clustername = \"condition\", size = 2): the method\nis not specified; by default, the method is srswor\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n   condition ID_unit      Prob\n1          1      16 0.6666667\n2          1      17 0.6666667\n3          1      18 0.6666667\n4          1      19 0.6666667\n5          1      20 0.6666667\n6          1      21 0.6666667\n7          1      22 0.6666667\n8          1      23 0.6666667\n9          1      24 0.6666667\n10         1      25 0.6666667\n11         1      26 0.6666667\n12         1      27 0.6666667\n13         3       1 0.6666667\n14         3       4 0.6666667\n15         3       5 0.6666667\n16         3       2 0.6666667\n17         3       3 0.6666667\n```\n:::\n:::\n\n\n# Handling Text Datasets\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"tm\")\nlibrary(tm)\ninstall.packages(\"SnowballC\") #tm uses SnowballC for stemming\n# read corpus\n# Documents is a folder in current working directory, holding some of Hillary's emails\ndocs <- Corpus(DirSource(\"Documents\"))\nmode(docs)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"list\"\n```\n:::\n:::\n\n\n## Inspect a document\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndocs[[20]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<PlainTextDocument>>\nMetadata:  7\nContent:  chars: 1150\n```\n:::\n:::\n\n\n## Preprocessing text\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndocs <- docs |>\n         tm_map(removePunctuation) |>\n         tm_map(content_transformer(tolower)) |> #to lower case\n         tm_map(removeNumbers) |>\n         tm_map(removeWords, stopwords(\"en\")) |> #stopwords, such as a, an.\n         tm_map(stripWhitespace) |>\n         tm_map(stemDocument) #e.g. computer -> comput\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(docs, removePunctuation): transformation drops\ndocuments\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(tm_map(docs, removePunctuation),\ncontent_transformer(tolower)): transformation drops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(tm_map(tm_map(docs, removePunctuation),\ncontent_transformer(tolower)), : transformation drops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(tm_map(tm_map(tm_map(docs, removePunctuation), :\ntransformation drops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(tm_map(tm_map(tm_map(tm_map(docs,\nremovePunctuation), : transformation drops documents\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in tm_map.SimpleCorpus(tm_map(tm_map(tm_map(tm_map(tm_map(docs, :\ntransformation drops documents\n```\n:::\n\n```{.r .cell-code}\ncontent(docs[[20]]) #note: stemming reduces a word to its â€˜rootâ€™ with the aassumption that the â€˜rootâ€™ represents the semantics of a word, e.g. computer, computing, computation, computers are about the concept of compute, which may be represented by â€˜computâ€™. but stemming is never perfect.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"unclassifi us depart state case f doc c date state dept produc hous select benghazi comm subject agreement sensit inform redact foia waiver releas full sent subject amaz sullivan jacob j sullivaniistategov wednesday septemb pm sherman wendi r h mill cheryl d nuland victoria j rein philipp re youth libya sherman wendi r sent wednesday septemb pm idrdintonemailcom hdrgclintonemailcom mill cheryl d sullivan jacob nuland victoria rein philipp subject fw youth libya hope see pictur kamala sent lakhdhir kamala sent wednesday septemb pm sherman wendi r cc escrogima ana grantham chris w subject youth libya libya hurra tv via facebook unclassifi us depart state case f doc c date state dept produc hous select benghazi comm subject agreement sensit inform redact foia waiver statescb\"\n```\n:::\n:::\n\n\nConvert text to a matrix using `TF*IDF scores` (see `TF*IDF` scores in Han's text):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDTData <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in TermDocumentMatrix.SimpleCorpus(x, control): custom functions are\nignored\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in weighting(x): empty document(s): 1931\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nDTData\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<DocumentTermMatrix (documents: 7945, terms: 42473)>>\nNon-/sparse entries: 885247/336562738\nSparsity           : 100%\nMaximal term length: 127\nWeighting          : term frequency - inverse document frequency (normalized) (tf-idf)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninspect(DTData[1:2, 1:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<DocumentTermMatrix (documents: 2, terms: 5)>>\nNon-/sparse entries: 7/3\nSparsity           : 30%\nMaximal term length: 9\nWeighting          : term frequency - inverse document frequency (normalized) (tf-idf)\nSample             :\n    Terms\nDocs  agreement    appreci        arm   benghazi       boss\n   1 0.09372950 0.06292734 0.06305152 0.12828049 0.09281579\n   2 0.03236792 0.00000000 0.00000000 0.05062803 0.00000000\n```\n:::\n:::\n\n\nCreate term-document matrix (also called inverted index, see Han's text in a later chapter):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTDData <- TermDocumentMatrix(docs, control = list(weighting = weightTfIdf))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in TermDocumentMatrix.SimpleCorpus(docs, control = list(weighting =\nweightTfIdf)): custom functions are ignored\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in weighting(x): empty document(s): 1931\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninspect(TDData[1:2, 1:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<TermDocumentMatrix (terms: 2, documents: 5)>>\nNon-/sparse entries: 6/4\nSparsity           : 40%\nMaximal term length: 9\nWeighting          : term frequency - inverse document frequency (normalized) (tf-idf)\nSample             :\n           Docs\nTerms                1          2          3          4          5\n  agreement 0.09372950 0.03236792 0.07881799 0.07580309 0.03906373\n  appreci   0.06292734 0.00000000 0.00000000 0.00000000 0.00000000\n```\n:::\n:::\n\n\n## Explore the dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfindFreqTerms(TDData, lowfreq = 75, highfreq = 1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"abedin\"          \"cheryl\"          \"huma\"            \"call\"           \n[5] \"abedinhstategov\"\n```\n:::\n:::\n\n\nFind correlations among terms:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfindAssocs(TDData, terms = \"bill\", corlimit = 0.25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$bill\nburn \n0.26 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfindAssocs(DTData, terms=c(\"bill\"), corlimit = 0.25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$bill\nburn \n0.26 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfindAssocs(DTData, terms=c(\"schedul\"), corlimit = 0.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$schedul\n            lona          valmoro valmoroustategov \n            0.39             0.38             0.38 \n```\n:::\n:::\n\n\n## Create a word cloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"wordcloud\")\ninstall.packages(\"RColorBrewer\")\nlibrary(wordcloud)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- as.matrix(TDData)\nfreq <- sort(rowSums(data), decreasing = TRUE)\nbase <-data.frame(word = names(freq), freq = freq)\n```\n:::\n\n\n`png()` opens a new device 'png' to output the graph to a local file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npng(file = \"wordCloud.png\", width = 1000, height = 700, bg= \"grey30\")\n\nwordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), \nrandom.order = FALSE, rot.per = 0.3, scale = c(1, .1))\n```\n:::\n\n\n`dev.off()` closed the `.png` file, now the current display is the default display in RStudio. Use `dev.list()` to find the graphics devices that are active, repeatedly use `dev.off()` to close devices that not needed. R Studio is the default display. When all other devices are closed, the default display is used.\n\nOutput the graph to the default display in RStudio\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), \nrandom.order = FALSE, rot.per = 0.3, scale = c(1, .1))\n```\n\n::: {.cell-output-display}\n![](rExercise2_files/figure-pdf/unnamed-chunk-132-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nCan you remove hrodclintonemailcom and redo the word cloud?\n\nSometimes you need to one-hot encoding a section of a dataframe. You can do it by using onehot package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"onehot\")\nlibrary(onehot)\nd <- data.frame(language=c(\"javascript\", \"python\", \"java\"), hours=c(10, 3, 5) )\nd$language <- as.factor(d$language) #convert the column to be encoded to Factor\nencoded <- onehot(d)\nnew_d <- predict(encoded, d)\nnew_d\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n     language=java language=javascript language=python hours\n[1,]             0                   1               0    10\n[2,]             0                   0               1     3\n[3,]             1                   0               0     5\n```\n:::\n:::\n\n\nOne hot encoding for data frame with multi-value cells (`language = \"javascript, python\"`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"qdapTools\")\nlibrary(qdapTools)\nd <- data.frame(language=c(\"javascript, python\", \"java\"), hours = c(3, 5) )\nd\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'qdapTools'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    id\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n            language hours\n1 javascript, python     3\n2               java     5\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndlist <- as.list(d)\nnew_d <- data.frame(cbind(dlist, mtabulate(strsplit(as.character(dlist$language), \", \")))) \n\nnew_d\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            language hours java javascript python\n1 javascript, python     3    0          1      1\n2               java     5    1          0      0\n```\n:::\n:::\n\n\n# \\[ADVANCED\\]\n\nExercises on your data set:\n\n1.  What attributes are there in your data set?\n\n2.  Do you have highly correlated attributes? How did you find out about the correlations or lack of correlations?\n\n3.  Do you have numerical attributes that you might want to bin? Try at least two methods and compare the differences.\n\n4.  If you have categorical attributes, use the concept hierarchy generation heuristics (based on attribute value counts) suggested in the textbook to produce some concept hierarchies. How well does this approach work for your attributes?\n",
    "supporting": [
      "rExercise2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{longtable}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{wrapfig}\n\\usepackage{float}\n\\usepackage{colortbl}\n\\usepackage{pdflscape}\n\\usepackage{tabu}\n\\usepackage{threeparttable}\n\\usepackage{threeparttablex}\n\\usepackage[normalem]{ulem}\n\\usepackage{makecell}\n\\usepackage{xcolor}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}