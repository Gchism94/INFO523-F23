{
  "hash": "e3ad06c25e19c4c9d9e9f3fe650fef81",
  "result": {
    "markdown": "---\ntitle: \"Classification: Alternative Techniques\"\n---\n\n\n## Install packages\n\nInstall the packages used in this chapter:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif(!require(pacman))\n  install.packages(\"pacman\")\n\npacman::p_load(\n  C50,                # C5.0 Decision Trees and Rule-Based Models\n  caret,              # Classification and Regression Training\n  e1071,              # Misc Functions of the Department of Statistics (e1071), TU Wien\n  keras,              # R Interface to 'Keras'\n  kernlab,            # Kernel-Based Machine Learning Lab\n  lattice,            # Trellis Graphics for R\n  MASS,               # Support Functions and Datasets for Venables and Ripley's MASS\n  mlbench,            # Machine Learning Benchmark Problems\n  nnet,               # Feedforward Neural Networks and Multinomial Log-Linear Models\n  palmerpenguins,     # Palmer Archipelago (Antarctica) Penguin Data\n  party,              # A Laboratory for Recursive Partytioning\n  partykit,           # A Toolkit for Recursive Partytioning\n  randomForest,       # Breiman and Cutler's Random Forests for Classification and Regression\n  rpart,              # Recursive partitioning models\n  RWeka,              # R/Weka Interface\n  scales,             # Scale Functions for Visualization\n  tidymodels,         # Tidy machine learning framework\n  tidyverse,          # Tidy data wrangling and visualization\n  xgboost             # Extreme Gradient Boosting\n)\n```\n:::\n\n\nShow fewer digits\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(digits=3)\n```\n:::\n\n\n## Introduction\n\nMany different [classification algorithms](https://en.wikipedia.org/wiki/Supervised_learning) have been proposed in the literature. In this chapter, we will apply some of the more popular methods.\n\n## Training and Test Data\n\nWe will use the Zoo dataset which is included in the R package `mlbench` (you may have to install it). The Zoo dataset containing 17 (mostly logical) variables on different 101 animals as a data frame with 17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize, type). We convert the data frame into a tidyverse tibble (optional).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Zoo, package=\"mlbench\")\nZoo <- as.data.frame(Zoo)\nZoo |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 101\nColumns: 17\n$ hair     <lgl> TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE…\n$ feathers <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ eggs     <lgl> FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, F…\n$ milk     <lgl> TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE…\n$ airborne <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ aquatic  <lgl> FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, F…\n$ predator <lgl> TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FAL…\n$ toothed  <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ backbone <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ breathes <lgl> TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE…\n$ venomous <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ fins     <lgl> FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, F…\n$ legs     <int> 4, 4, 0, 4, 4, 4, 4, 0, 0, 4, 4, 2, 0, 0, 4, 6, 2, 4, 0, 0, 2…\n$ tail     <lgl> FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE…\n$ domestic <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, …\n$ catsize  <lgl> TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALS…\n$ type     <fct> mammal, mammal, fish, mammal, mammal, mammal, mammal, fish, f…\n```\n:::\n:::\n\n\nWe will use the package [**caret**](https://topepo.github.io/caret/) to make preparing training sets and building classification (and regression) models easier. A great cheat sheet can be found [here](https://ugoproto.github.io/ugo_r_doc/pdf/caret.pdf).\n\nMulti-core support can be used for cross-validation. **Note:** It is commented out here because it does not work with rJava used in RWeka below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##library(doMC, quietly = TRUE)\n##registerDoMC(cores = 4)\n##getDoParWorkers()\n```\n:::\n\n\nTest data is not used in the model building process and needs to be set aside purely for testing the model after it is completely built. Here I use 80% for training.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)  # for reproducibility\ninTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]\nZoo_train <- dplyr::slice(Zoo, inTrain)\nZoo_test <- dplyr::slice(Zoo, -inTrain)\n```\n:::\n\n\n## Fitting Different Classification Models to the Training Data\n\nCreate a fixed sampling scheme (10-folds) so we can compare the fitted models later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_index <- createFolds(Zoo_train$type, k = 10)\n```\n:::\n\n\nThe fixed folds are used in `train()` with the argument `trControl = trainControl(method = \"cv\", indexOut = train_index))`. If you don't need fixed folds, then remove `indexOut = train_index` in the code below.\n\nFor help with building models in caret see: `? train`\n\n**Note:** Be careful if you have many `NA` values in your data. `train()` and cross-validation many fail in some cases. If that is the case then you can remove features (columns) which have many `NA`s, omit `NA`s using `na.omit()` or use imputation to replace them with reasonable values (e.g., by the feature mean or via kNN). Highly imbalanced datasets are also problematic since there is a chance that a fold does not contain examples of each class leading to a hard to understand error message.\n\n### Conditional Inference Tree (Decision Tree)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctreeFit <- Zoo_train |> train(type ~ .,\n  method = \"ctree\",\n  data = _,\n\ttuneLength = 5,\n\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\nctreeFit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConditional Inference Tree \n\n83 samples\n16 predictors\n 7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 76, 72, 73, 76, 75, 75, ... \nResampling results across tuning parameters:\n\n  mincriterion  Accuracy  Kappa\n  0.010         0.827     0.772\n  0.255         0.827     0.772\n  0.500         0.827     0.772\n  0.745         0.827     0.772\n  0.990         0.827     0.772\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mincriterion = 0.99.\n```\n:::\n\n```{.r .cell-code}\nplot(ctreeFit$finalModel)\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### C 4.5 Decision Tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC45Fit <- Zoo_train |> train(type ~ .,\n  method = \"J48\",\n  data = _,\n\ttuneLength = 5,\n\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\nC45Fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nC4.5-like Trees \n\n83 samples\n16 predictors\n 7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 76, 75, 73, 76, 74, 74, ... \nResampling results across tuning parameters:\n\n  C      M  Accuracy  Kappa\n  0.010  1  0.975     0.967\n  0.010  2  0.965     0.954\n  0.010  3  0.953     0.940\n  0.010  4  0.959     0.948\n  0.010  5  0.970     0.962\n  0.133  1  1.000     1.000\n  0.133  2  0.976     0.968\n  0.133  3  0.965     0.954\n  0.133  4  0.959     0.948\n  0.133  5  0.970     0.962\n  0.255  1  1.000     1.000\n  0.255  2  0.976     0.968\n  0.255  3  0.965     0.954\n  0.255  4  0.959     0.948\n  0.255  5  0.970     0.962\n  0.378  1  1.000     1.000\n  0.378  2  0.976     0.968\n  0.378  3  0.965     0.954\n  0.378  4  0.959     0.948\n  0.378  5  0.970     0.962\n  0.500  1  1.000     1.000\n  0.500  2  0.976     0.968\n  0.500  3  0.965     0.954\n  0.500  4  0.959     0.948\n  0.500  5  0.970     0.962\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were C = 0.133 and M = 1.\n```\n:::\n\n```{.r .cell-code}\nC45Fit$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJ48 pruned tree\n------------------\n\nfeathersTRUE <= 0\n|   milkTRUE <= 0\n|   |   backboneTRUE <= 0\n|   |   |   predatorTRUE <= 0\n|   |   |   |   legs <= 2: mollusc.et.al (1.0)\n|   |   |   |   legs > 2: insect (6.0)\n|   |   |   predatorTRUE > 0: mollusc.et.al (8.0/1.0)\n|   |   backboneTRUE > 0\n|   |   |   finsTRUE <= 0\n|   |   |   |   aquaticTRUE <= 0: reptile (3.0)\n|   |   |   |   aquaticTRUE > 0\n|   |   |   |   |   eggsTRUE <= 0: reptile (1.0)\n|   |   |   |   |   eggsTRUE > 0: amphibian (4.0)\n|   |   |   finsTRUE > 0: fish (11.0)\n|   milkTRUE > 0: mammal (33.0)\nfeathersTRUE > 0: bird (16.0)\n\nNumber of Leaves  : \t9\n\nSize of the tree : \t17\n```\n:::\n:::\n\n\n### K-Nearest Neighbors\n\n**Note:** kNN uses Euclidean distance, so data should be standardized (scaled) first. Here legs are measured between 0 and 6 while all other variables are between 0 and 1. Scaling can be directly performed as preprocessing in `train` using the parameter `preProcess = \"scale\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnFit <- Zoo_train |> train(type ~ .,\n  method = \"knn\",\n  data = _,\n  preProcess = \"scale\",\n\ttuneLength = 5,\n  tuneGrid=data.frame(k = 1:10),\n\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\nknnFit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk-Nearest Neighbors \n\n83 samples\n16 predictors\n 7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n\nPre-processing: scaled (16) \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 77, 74, 75, 75, 74, 74, ... \nResampling results across tuning parameters:\n\n  k   Accuracy  Kappa\n   1  1.000     1.000\n   2  0.965     0.954\n   3  0.963     0.951\n   4  0.942     0.925\n   5  0.941     0.921\n   6  0.963     0.951\n   7  0.963     0.951\n   8  0.941     0.921\n   9  0.908     0.883\n  10  0.918     0.892\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 1.\n```\n:::\n\n```{.r .cell-code}\nknnFit$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1-nearest neighbor model\nTraining set outcome distribution:\n\n       mammal          bird       reptile          fish     amphibian \n           33            16             4            11             4 \n       insect mollusc.et.al \n            7             8 \n```\n:::\n:::\n\n\n### PART (Rule-based classifier)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrulesFit <- Zoo_train |> train(type ~ .,\n  method = \"PART\",\n  data = _,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\", indexOut = train_index))\nrulesFit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRule-Based Classifier \n\n83 samples\n16 predictors\n 7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 73, 74, 76, 76, 75, 74, ... \nResampling results across tuning parameters:\n\n  threshold  pruned  Accuracy  Kappa\n  0.010      yes     0.979     0.973\n  0.010      no      0.979     0.973\n  0.133      yes     0.990     0.987\n  0.133      no      0.979     0.973\n  0.255      yes     0.990     0.987\n  0.255      no      0.979     0.973\n  0.378      yes     0.990     0.987\n  0.378      no      0.979     0.973\n  0.500      yes     0.990     0.987\n  0.500      no      0.979     0.973\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were threshold = 0.5 and pruned = yes.\n```\n:::\n\n```{.r .cell-code}\nrulesFit$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPART decision list\n------------------\n\nfeathersTRUE <= 0 AND\nmilkTRUE > 0: mammal (33.0)\n\nfeathersTRUE > 0: bird (16.0)\n\nbackboneTRUE <= 0 AND\nairborneTRUE <= 0 AND\npredatorTRUE > 0: mollusc.et.al (7.0)\n\nbackboneTRUE > 0 AND\nfinsTRUE > 0: fish (11.0)\n\nbackboneTRUE <= 0: insect (8.0/1.0)\n\naquaticTRUE > 0: amphibian (5.0/1.0)\n\n: reptile (3.0)\n\nNumber of Rules  : \t7\n```\n:::\n:::\n\n\n### Linear Support Vector Machines\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvmFit <- Zoo_train |> train(type ~.,\n  method = \"svmLinear\",\n  data = _,\n\ttuneLength = 5,\n\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\nsvmFit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSupport Vector Machines with Linear Kernel \n\n83 samples\n16 predictors\n 7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 74, 74, 77, 75, 74, 77, ... \nResampling results:\n\n  Accuracy  Kappa\n  1         1    \n\nTuning parameter 'C' was held constant at a value of 1\n```\n:::\n\n```{.r .cell-code}\nsvmFit$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 1 \n\nLinear (vanilla) kernel function. \n\nNumber of Support Vectors : 39 \n\nObjective Function Value : -0.143 -0.217 -0.15 -0.175 -0.0934 -0.0974 -0.292 -0.0835 -0.154 -0.0901 -0.112 -0.189 -0.593 -0.13 -0.179 -0.122 -0.0481 -0.0838 -0.125 -0.15 -0.501 \nTraining error : 0 \n```\n:::\n:::\n\n\n### Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrandomForestFit <- Zoo_train |> train(type ~ .,\n  method = \"rf\",\n  data = _,\n\ttuneLength = 5,\n\ttrControl = trainControl(method = \"cv\", indexOut = train_index))\nrandomForestFit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest \n\n83 samples\n16 predictors\n 7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 75, 76, 75, 76, 74, 73, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy  Kappa\n   2    1         1    \n   5    1         1    \n   9    1         1    \n  12    1         1    \n  16    1         1    \n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n```\n:::\n\n```{.r .cell-code}\nrandomForestFit$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n randomForest(x = x, y = y, mtry = param$mtry) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 7.23%\nConfusion matrix:\n              mammal bird reptile fish amphibian insect mollusc.et.al\nmammal            33    0       0    0         0      0             0\nbird               0   16       0    0         0      0             0\nreptile            0    1       0    2         1      0             0\nfish               0    0       0   11         0      0             0\namphibian          0    0       0    0         4      0             0\ninsect             0    0       0    0         0      7             0\nmollusc.et.al      1    0       0    0         0      1             6\n              class.error\nmammal               0.00\nbird                 0.00\nreptile              1.00\nfish                 0.00\namphibian            0.00\ninsect               0.00\nmollusc.et.al        0.25\n```\n:::\n:::\n\n\n### Gradient Boosted Decision Trees (xgboost)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgboostFit <- Zoo_train |> train(type ~ .,\n  method = \"xgbTree\",\n  data = _,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\", indexOut = train_index),\n  tuneGrid = expand.grid(\n    nrounds = 20,\n    max_depth = 3,\n    colsample_bytree = .6,\n    eta = 0.1,\n    gamma=0,\n    min_child_weight = 1,\n    subsample = .5\n  ))\nxgboostFit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neXtreme Gradient Boosting \n\n83 samples\n16 predictors\n 7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 77, 73, 74, 75, 75, 75, ... \nResampling results:\n\n  Accuracy  Kappa\n  0.973     0.964\n\nTuning parameter 'nrounds' was held constant at a value of 20\nTuning\n held constant at a value of 1\nTuning parameter 'subsample' was held\n constant at a value of 0.5\n```\n:::\n\n```{.r .cell-code}\nxgboostFit$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n##### xgb.Booster\nraw: 112.4 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = param$eta, max_depth = param$max_depth, \n    gamma = param$gamma, colsample_bytree = param$colsample_bytree, \n    min_child_weight = param$min_child_weight, subsample = param$subsample), \n    data = x, nrounds = param$nrounds, num_class = length(lev), \n    objective = \"multi:softprob\")\nparams (as set within xgb.train):\n  eta = \"0.1\", max_depth = \"3\", gamma = \"0\", colsample_bytree = \"0.6\", min_child_weight = \"1\", subsample = \"0.5\", num_class = \"7\", objective = \"multi:softprob\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.print.evaluation(period = print_every_n)\n# of features: 16 \nniter: 20\nnfeatures : 16 \nxNames : hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \nproblemType : Classification \ntuneValue :\n\t  nrounds max_depth eta gamma colsample_bytree min_child_weight subsample\n1      20         3 0.1     0              0.6                1       0.5\nobsLevels : mammal bird reptile fish amphibian insect mollusc.et.al \nparam :\n\tlist()\n```\n:::\n:::\n\n\n### Artificial Neural Network\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnnetFit <- Zoo_train |> train(type ~ .,\n  method = \"nnet\",\n  data = _,\n\ttuneLength = 5,\n\ttrControl = trainControl(method = \"cv\", indexOut = train_index),\n  trace = FALSE)\nnnetFit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNeural Network \n\n83 samples\n16 predictors\n 7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 75, 74, 74, 74, 74, 75, ... \nResampling results across tuning parameters:\n\n  size  decay  Accuracy  Kappa\n  1     0e+00  0.776     0.681\n  1     1e-04  0.789     0.709\n  1     1e-03  0.911     0.882\n  1     1e-02  0.832     0.781\n  1     1e-01  0.722     0.621\n  3     0e+00  0.963     0.950\n  3     1e-04  0.976     0.968\n  3     1e-03  0.986     0.979\n  3     1e-02  0.986     0.981\n  3     1e-01  0.976     0.968\n  5     0e+00  0.965     0.953\n  5     1e-04  0.986     0.981\n  5     1e-03  0.986     0.981\n  5     1e-02  0.986     0.981\n  5     1e-01  0.986     0.981\n  7     0e+00  0.976     0.968\n  7     1e-04  0.986     0.981\n  7     1e-03  0.986     0.981\n  7     1e-02  0.986     0.981\n  7     1e-01  0.986     0.981\n  9     0e+00  0.986     0.981\n  9     1e-04  0.986     0.981\n  9     1e-03  0.986     0.981\n  9     1e-02  0.986     0.981\n  9     1e-01  0.986     0.981\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were size = 3 and decay = 0.01.\n```\n:::\n\n```{.r .cell-code}\nnnetFit$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na 16-3-7 network with 79 weights\ninputs: hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \noutput(s): .outcome \noptions were - softmax modelling  decay=0.01\n```\n:::\n:::\n\n\n## Comparing Models\n\nCollect the performance metrics from the models trained on the same data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresamps <- resamples(list(\n  ctree = ctreeFit,\n  C45 = C45Fit,\n  SVM = svmFit,\n  KNN = knnFit,\n  rules = rulesFit,\n  randomForest = randomForestFit,\n  xgboost = xgboostFit,\n  NeuralNet = nnetFit\n    ))\nresamps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nresamples.default(x = list(ctree = ctreeFit, C45 = C45Fit, SVM = svmFit, KNN\n = knnFit, rules = rulesFit, randomForest = randomForestFit, xgboost\n = xgboostFit, NeuralNet = nnetFit))\n\nModels: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet \nNumber of resamples: 10 \nPerformance metrics: Accuracy, Kappa \nTime estimates for: everything, final model fit \n```\n:::\n:::\n\n\nCalculate summary statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(resamps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsummary.resamples(object = resamps)\n\nModels: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet \nNumber of resamples: 10 \n\nAccuracy \n              Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\nctree        0.700   0.778  0.817 0.827   0.871    1    0\nC45          1.000   1.000  1.000 1.000   1.000    1    0\nSVM          1.000   1.000  1.000 1.000   1.000    1    0\nKNN          1.000   1.000  1.000 1.000   1.000    1    0\nrules        0.900   1.000  1.000 0.990   1.000    1    0\nrandomForest 1.000   1.000  1.000 1.000   1.000    1    0\nxgboost      0.857   1.000  1.000 0.973   1.000    1    0\nNeuralNet    0.857   1.000  1.000 0.986   1.000    1    0\n\nKappa \n              Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\nctree        0.634   0.715  0.748 0.772   0.815    1    0\nC45          1.000   1.000  1.000 1.000   1.000    1    0\nSVM          1.000   1.000  1.000 1.000   1.000    1    0\nKNN          1.000   1.000  1.000 1.000   1.000    1    0\nrules        0.868   1.000  1.000 0.987   1.000    1    0\nrandomForest 1.000   1.000  1.000 1.000   1.000    1    0\nxgboost      0.806   1.000  1.000 0.964   1.000    1    0\nNeuralNet    0.806   1.000  1.000 0.981   1.000    1    0\n```\n:::\n\n```{.r .cell-code}\nlibrary(lattice)\nbwplot(resamps, layout = c(3, 1))\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nPerform inference about differences between models. For each metric, all pair-wise differences are computed and tested to assess if the difference is equal to zero. By default Bonferroni correction for multiple comparison is used. Differences are shown in the upper triangle and p-values are in the lower triangle.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndifs <- diff(resamps)\ndifs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\ndiff.resamples(x = resamps)\n\nModels: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet \nMetrics: Accuracy, Kappa \nNumber of differences: 28 \np-value adjustment: bonferroni \n```\n:::\n\n```{.r .cell-code}\nsummary(difs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsummary.diff.resamples(object = difs)\n\np-value adjustment: bonferroni \nUpper diagonal: estimates of the difference\nLower diagonal: p-value for H0: difference = 0\n\nAccuracy \n             ctree   C45      SVM      KNN      rules    randomForest xgboost \nctree                -0.17262 -0.17262 -0.17262 -0.16262 -0.17262     -0.14583\nC45          0.00193           0.00000  0.00000  0.01000  0.00000      0.02679\nSVM          0.00193 NA                 0.00000  0.01000  0.00000      0.02679\nKNN          0.00193 NA       NA                 0.01000  0.00000      0.02679\nrules        0.00376 1.00000  1.00000  1.00000           -0.01000      0.01679\nrandomForest 0.00193 NA       NA       NA       1.00000                0.02679\nxgboost      0.05129 1.00000  1.00000  1.00000  1.00000  1.00000              \nNeuralNet    0.01405 1.00000  1.00000  1.00000  1.00000  1.00000      1.00000 \n             NeuralNet\nctree        -0.15833 \nC45           0.01429 \nSVM           0.01429 \nKNN           0.01429 \nrules         0.00429 \nrandomForest  0.01429 \nxgboost      -0.01250 \nNeuralNet             \n\nKappa \n             ctree   C45      SVM      KNN      rules    randomForest xgboost \nctree                -0.22840 -0.22840 -0.22840 -0.21524 -0.22840     -0.19229\nC45          0.00116           0.00000  0.00000  0.01316  0.00000      0.03611\nSVM          0.00116 NA                 0.00000  0.01316  0.00000      0.03611\nKNN          0.00116 NA       NA                 0.01316  0.00000      0.03611\nrules        0.00238 1.00000  1.00000  1.00000           -0.01316      0.02295\nrandomForest 0.00116 NA       NA       NA       1.00000                0.03611\nxgboost      0.04216 1.00000  1.00000  1.00000  1.00000  1.00000              \nNeuralNet    0.01055 1.00000  1.00000  1.00000  1.00000  1.00000      1.00000 \n             NeuralNet\nctree        -0.20895 \nC45           0.01944 \nSVM           0.01944 \nKNN           0.01944 \nrules         0.00629 \nrandomForest  0.01944 \nxgboost      -0.01667 \nNeuralNet             \n```\n:::\n:::\n\n\nAll perform similarly well except ctree (differences in the first row are negative and the p-values in the first column are \\<.05 indicating that the null-hypothesis of a difference of 0 can be rejected).\n\n## Applying the Chosen Model to the Test Data\n\nMost models do similarly well on the data. We choose here the random forest model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npr <- predict(randomForestFit, Zoo_test)\npr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] mammal        mammal        mammal        fish          fish         \n [6] bird          bird          mammal        mammal        mammal       \n[11] mammal        mollusc.et.al reptile       mammal        bird         \n[16] mollusc.et.al bird          insect       \nLevels: mammal bird reptile fish amphibian insect mollusc.et.al\n```\n:::\n:::\n\n\nCalculate the confusion matrix for the held-out test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(pr, reference = Zoo_test$type)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n               Reference\nPrediction      mammal bird reptile fish amphibian insect mollusc.et.al\n  mammal             8    0       0    0         0      0             0\n  bird               0    4       0    0         0      0             0\n  reptile            0    0       1    0         0      0             0\n  fish               0    0       0    2         0      0             0\n  amphibian          0    0       0    0         0      0             0\n  insect             0    0       0    0         0      1             0\n  mollusc.et.al      0    0       0    0         0      0             2\n\nOverall Statistics\n                                    \n               Accuracy : 1         \n                 95% CI : (0.815, 1)\n    No Information Rate : 0.444     \n    P-Value [Acc > NIR] : 4.58e-07  \n                                    \n                  Kappa : 1         \n                                    \n Mcnemar's Test P-Value : NA        \n\nStatistics by Class:\n\n                     Class: mammal Class: bird Class: reptile Class: fish\nSensitivity                  1.000       1.000         1.0000       1.000\nSpecificity                  1.000       1.000         1.0000       1.000\nPos Pred Value               1.000       1.000         1.0000       1.000\nNeg Pred Value               1.000       1.000         1.0000       1.000\nPrevalence                   0.444       0.222         0.0556       0.111\nDetection Rate               0.444       0.222         0.0556       0.111\nDetection Prevalence         0.444       0.222         0.0556       0.111\nBalanced Accuracy            1.000       1.000         1.0000       1.000\n                     Class: amphibian Class: insect Class: mollusc.et.al\nSensitivity                        NA        1.0000                1.000\nSpecificity                         1        1.0000                1.000\nPos Pred Value                     NA        1.0000                1.000\nNeg Pred Value                     NA        1.0000                1.000\nPrevalence                          0        0.0556                0.111\nDetection Rate                      0        0.0556                0.111\nDetection Prevalence                0        0.0556                0.111\nBalanced Accuracy                  NA        1.0000                1.000\n```\n:::\n:::\n\n\n## Comparing Decision Boundaries of Popular Classification Techniques\n\nClassifiers create decision boundaries to discriminate between classes. Different classifiers are able to create different shapes of decision boundaries (e.g., some are strictly linear) and thus some classifiers may perform better for certain datasets. This page visualizes the decision boundaries found by several popular classification methods.\n\nThe following plot adds the decision boundary (black lines) and classification confidence (color intensity) by evaluating the classifier at evenly spaced grid points. Note that low resolution (to make evaluation faster) will make the decision boundary look like it has small steps even if it is a (straight) line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(scales)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(caret)\n\ndecisionplot <- function(model, data, class_var, \n  predict_type = c(\"class\", \"prob\"), resolution = 3 * 72) {\n  # resolution is set to 72 dpi if the image is rendered  3 inches wide. \n  \n  y <- data |> pull(class_var)\n  x <- data |> dplyr::select(-all_of(class_var))\n  \n  # resubstitution accuracy\n  prediction <- predict(model, x, type = predict_type[1])\n  # LDA returns a list\n  if(is.list(prediction)) prediction <- prediction$class\n  prediction <- factor(prediction, levels = levels(y))\n  \n  cm <- confusionMatrix(data = prediction, \n                        reference = y)\n  acc <- cm$overall[\"Accuracy\"]\n  \n  # evaluate model on a grid\n  r <- sapply(x[, 1:2], range, na.rm = TRUE)\n  xs <- seq(r[1,1], r[2,1], length.out = resolution)\n  ys <- seq(r[1,2], r[2,2], length.out = resolution)\n  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))\n  colnames(g) <- colnames(r)\n  g <- as_tibble(g)\n  \n  ### guess how to get class labels from predict\n  ### (unfortunately not very consistent between models)\n  cl <- predict(model, g, type = predict_type[1])\n  \n  # LDA returns a list\n  prob <- NULL\n  if(is.list(cl)) { \n    prob <- cl$posterior\n    cl <- cl$class\n  } else\n    if(!is.na(predict_type[2]))\n      try(prob <- predict(model, g, type = predict_type[2]))\n  \n  # we visualize the difference in probability/score between the \n  # winning class and the second best class.\n  # don't use probability if predict for the classifier does not support it.\n  max_prob <- 1\n  if(!is.null(prob))\n    try({\n      max_prob <- t(apply(prob, MARGIN = 1, sort, decreasing = TRUE))\n      max_prob <- max_prob[,1] - max_prob[,2]\n    }, silent = TRUE) \n  \n  cl <- factor(cl, levels = levels(y))\n  \n  g <- g |> add_column(prediction = cl, probability = max_prob)\n  \n  ggplot(g, mapping = aes(\n    x = .data[[colnames(g)[1]]], y = .data[[colnames(g)[2]]])) +\n    geom_raster(mapping = aes(fill = prediction, alpha = probability)) +\n    geom_contour(mapping = aes(z = as.numeric(prediction)), \n      bins = length(levels(cl)), linewidth = .5, color = \"black\") +\n    geom_point(data = data, mapping =  aes(\n      x = .data[[colnames(data)[1]]], \n      y = .data[[colnames(data)[2]]],\n      shape = .data[[class_var]]), alpha = .7) + \n    scale_alpha_continuous(range = c(0,1), limits = c(0,1), guide = \"none\") +  \n    labs(subtitle = paste(\"Training accuracy:\", round(acc, 2))) +\n     theme_minimal(base_size = 14)\n}\n```\n:::\n\n\n### Penguins Dataset\n\nFor easier visualization, we use two dimensions of the `penguins` dataset. Contour lines visualize the density like mountains on a map.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1000)\ndata(\"penguins\")\npenguins <- as_tibble(penguins) |>\n  drop_na()\n\n### Three classes \n### (note: MASS also has a select function which hides dplyr's select)\nx <- penguins |> dplyr::select(bill_length_mm, bill_depth_mm, species)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 333 × 3\n   bill_length_mm bill_depth_mm species\n            <dbl>         <dbl> <fct>  \n 1           39.1          18.7 Adelie \n 2           39.5          17.4 Adelie \n 3           40.3          18   Adelie \n 4           36.7          19.3 Adelie \n 5           39.3          20.6 Adelie \n 6           38.9          17.8 Adelie \n 7           39.2          19.6 Adelie \n 8           41.1          17.6 Adelie \n 9           38.6          21.2 Adelie \n10           34.6          21.1 Adelie \n# ℹ 323 more rows\n```\n:::\n\n```{.r .cell-code}\nggplot(x, aes(x = bill_length_mm, y = bill_depth_mm, fill = species)) +  \n  stat_density_2d(geom = \"polygon\", aes(alpha = after_stat(level))) +\n  geom_point() +\n  theme_minimal(base_size = 14) +\n  labs(x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       fill = \"Species\",\n       alpha = \"Density\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n*Note:* There is some overplotting and you could use `geom_jitter()` instead of `geom_point()`.\n\n#### K-Nearest Neighbors Classifier\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> caret::knn3(species ~ ., data = _, k = 1)\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"kNN (1 neighbor)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> caret::knn3(species ~ ., data = _, k = 3)\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"kNN (3 neighbor)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-21-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> caret::knn3(species ~ ., data = _, k = 9)\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"kNN (9 neighbor)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-21-3.png){width=672}\n:::\n:::\n\n\nIncreasing $k$ smooths the decision boundary. At $k=1$, we see white areas around points where penguins of two classes are in the same spot. Here, the algorithm randomly chooses a class during prediction resulting in the meandering decision boundary. The predictions in that area are not stable and every time we ask for a class, we may get a different class.\n\n#### Naive Bayes Classifier\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> e1071::naiveBayes(species ~ ., data = _)\ndecisionplot(model, x, class_var = \"species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"Naive Bayes\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\") \n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n#### Linear Discriminant Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> MASS::lda(species ~ ., data = _)\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"LDA\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n#### Multinomial Logistic Regression (implemented in nnet)\n\nMultinomial logistic regression is an extension of logistic regression to problems with more than two classes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> nnet::multinom(species ~., data = _)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  12 (6 variable)\ninitial  value 365.837892 \niter  10 value 26.650783\niter  20 value 23.943597\niter  30 value 23.916873\niter  40 value 23.901339\niter  50 value 23.895442\niter  60 value 23.894251\nfinal  value 23.892065 \nconverged\n```\n:::\n\n```{.r .cell-code}\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"Multinomial Logistic Regression\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n#### Decision Trees\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> rpart::rpart(species ~ ., data = _)\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"CART\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> rpart::rpart(species ~ ., data = _,\n  control = rpart.control(cp = 0.001, minsplit = 1))\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"CART (overfitting)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> C50::C5.0(species ~ ., data = _)\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"C5.0\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-25-3.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> randomForest::randomForest(species ~ ., data = _)\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"Random Forest\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-25-4.png){width=672}\n:::\n:::\n\n\n#### SVM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> e1071::svm(species ~ ., data = _, kernel = \"linear\")\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"SVM (linear kernel)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> e1071::svm(species ~ ., data = _, kernel = \"radial\")\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"SVM (radial kernel)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-26-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> e1071::svm(species ~ ., data = _, kernel = \"polynomial\")\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"SVM (polynomial kernel)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-26-3.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> e1071::svm(species ~ ., data = _, kernel = \"sigmoid\")\ndecisionplot(model, x, class_var = \"species\") + \n  labs(title = \"SVM (sigmoid kernel)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-26-4.png){width=672}\n:::\n:::\n\n\n#### Single Layer Feed-forward Neural Networks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <-x |> nnet::nnet(species ~ ., data = _, size = 1, trace = FALSE)\ndecisionplot(model, x, class_var  = \"species\", \n  predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"NN (1 neuron)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Computation failed in `stat_contour()`\nCaused by error in `if (zero_range(range)) ...`:\n! missing value where TRUE/FALSE needed\n```\n:::\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <-x |> nnet::nnet(species ~ ., data = _, size = 2, trace = FALSE)\ndecisionplot(model, x, class_var  = \"species\", \n  predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"NN (2 neurons)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Computation failed in `stat_contour()`\nCaused by error in `if (zero_range(range)) ...`:\n! missing value where TRUE/FALSE needed\n```\n:::\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-27-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <-x |> nnet::nnet(species ~ ., data = _, size = 4, trace = FALSE)\ndecisionplot(model, x, class_var  = \"species\", \n  predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"NN (4 neurons)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-27-3.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <-x |> nnet::nnet(species ~ ., data = _, size = 10, trace = FALSE)\ndecisionplot(model, x, class_var  = \"species\", \n  predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"NN (10 neurons)\",\n       x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\",\n       shape = \"Species\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-27-4.png){width=672}\n:::\n:::\n\n\n### Circle Dataset\n\nThis set is not linearly separable!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1000)\n\nx <- mlbench::mlbench.circle(500)\n###x <- mlbench::mlbench.cassini(500)\n###x <- mlbench::mlbench.spirals(500, sd = .1)\n###x <- mlbench::mlbench.smiley(500)\nx <- cbind(as.data.frame(x$x), factor(x$classes))\ncolnames(x) <- c(\"x\", \"y\", \"class\")\nx <- as_tibble(x)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 3\n         x       y class\n     <dbl>   <dbl> <fct>\n 1 -0.344   0.448  1    \n 2  0.518   0.915  2    \n 3 -0.772  -0.0913 1    \n 4  0.382   0.412  1    \n 5  0.0328  0.438  1    \n 6 -0.865  -0.354  2    \n 7  0.477   0.640  2    \n 8  0.167  -0.809  2    \n 9 -0.568  -0.281  1    \n10 -0.488   0.638  2    \n# ℹ 490 more rows\n```\n:::\n\n```{.r .cell-code}\nggplot(x, aes(x = x, y = y, color = class)) + \n  geom_point() +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n#### K-Nearest Neighbors Classifier\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> caret::knn3(class ~ ., data = _, k = 1)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"kNN (1 neighbor)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> caret::knn3(class ~ ., data = _, k = 10)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"kNN (10 neighbor)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-29-2.png){width=672}\n:::\n:::\n\n\n#### Naive Bayes Classifier\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> e1071::naiveBayes(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"naive Bayes\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n#### Linear Discriminant Analysis\n\nLDA cannot find a good model since the true decision boundary is not linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> MASS::lda(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"LDA\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n#### Logistic Regression (implemented in nnet)\n\nMultinomial logistic regression is an extension of logistic regression to problems with more than two classes. It also tries to find a linear decision boundary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> nnet::multinom(class ~., data = _)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  4 (3 variable)\ninitial  value 346.573590 \nfinal  value 346.308371 \nconverged\n```\n:::\n\n```{.r .cell-code}\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"Multinomial Logistic Regression\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n#### Decision Trees\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> rpart::rpart(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"CART\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> rpart::rpart(class ~ ., data = _,\n  control = rpart.control(cp = 0.001, minsplit = 1))\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"CART (overfitting)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-33-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> C50::C5.0(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"C5.0\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-33-3.png){width=672}\n:::\n\n```{.r .cell-code}\nlibrary(randomForest)\nmodel <- x |> randomForest(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"Random Forest\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-33-4.png){width=672}\n:::\n:::\n\n\n#### SVM\n\nLinear SVM does not work for this data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"linear\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (linear kernel)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Computation failed in `stat_contour()`\nCaused by error in `if (zero_range(range)) ...`:\n! missing value where TRUE/FALSE needed\n```\n:::\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"radial\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (radial kernel)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-34-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"polynomial\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (polynomial kernel)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-34-3.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"sigmoid\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (sigmoid kernel)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-34-4.png){width=672}\n:::\n:::\n\n\n#### Single Layer Feed-forward Neural Networks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 1, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + \n  labs(title = \"NN (1 neuron)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 2, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + \n  labs(title = \"NN (2 neurons)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-35-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 4, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + \n  labs(title = \"NN (4 neurons)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-35-3.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 10, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + \n  labs(title = \"NN (10 neurons)\",\n       shape = \"Class\",\n       fill = \"Prediction\")\n```\n\n::: {.cell-output-display}\n![](rExercise3_1_files/figure-html/unnamed-chunk-35-4.png){width=672}\n:::\n:::\n\n\n## More Information on Classification with R\n\n-   Package caret: <http://topepo.github.io/caret/index.html>\n-   Tidymodels (machine learning with tidyverse): <https://www.tidymodels.org/>\n-   R taskview on machine learning: <http://cran.r-project.org/web/views/MachineLearning.html>\n",
    "supporting": [
      "rExercise3_1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}