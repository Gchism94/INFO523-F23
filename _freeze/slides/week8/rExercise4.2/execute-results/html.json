{
  "hash": "3b9023f75af6c78a30b646044fd9caf5",
  "result": {
    "markdown": "---\ntitle: Regression in R\n---\n\n::: {.cell}\n\n:::\n\n\nThe following tutorial contains R examples for solving regression problems.\n\nRegression is a modeling technique for predicting quantitative-valued target attributes. The goals for this tutorial are as follows: 1. To provide examples of using different regression methods from the tidymodels package. 2. To demonstrate the problem of model overfitting due to correlated attributes in the data. 3. To illustrate how regularization can be used to avoid model overfitting.\n\nRead the step-by-step instructions below carefully. To execute the code, click on the corresponding cell and press the SHIFT-ENTER keys simultaneously.\n\n## **Synthetic Data Generation**\n\nTo illustrate how linear regression works, we first generate a random 1-dimensional vector of predictor variables, x, from a uniform distribution. The response variable y has a linear relationship with x according to the following equation: y = -3x + 1 + epsilon, where epsilon corresponds to random noise sampled from a Gaussian distribution with mean 0 and standard deviation of 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameters\nseed <- 1            # seed for random number generation \nnumInstances <- 200  # number of data instances\n\n# Set seed\nset.seed(seed)\n\n# Generate data\nX <- matrix(runif(numInstances), ncol=1)\ny_true <- -3*X + 1 \ny <- y_true + matrix(rnorm(numInstances), ncol=1)\n\n# Plot\nggplot() +\n  geom_point(aes(x=X, y=y), color=\"black\") +\n  geom_line(aes(x=X, y=y_true), color=\"blue\", linewidth=1) +\n  ggtitle('True function: y = -3X + 1') +\n  xlab('X') +\n  ylab('y')\n```\n\n::: {.cell-output-display}\n![](rExercise4.2_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## **Multiple Linear Regression**\n\nIn this example, we illustrate how to use Python scikit-learn package to fit a multiple linear regression (MLR) model. Given a training set ${X,y}$ MLR is designed to learn the regression function $f(X,w) = X^T w + w_0$ by minimizing the following loss function given a training set $\\{X_i,y_i\\}_{i=1}^N$:\n\n$$\nL(y,f(X,w)) = \\sum_{i=1}^N \\|y_i - X_i w - w_0\\|^2,\n$$\n\nwhere $w$ (slope) and $w_0$ (intercept) are the regression coefficients.\n\nGiven the input dataset, the following steps are performed: 1. Split the input data into their respective training and test sets. 2. Fit multiple linear regression to the training data. 3. Apply the model to the test data. 4. Evaluate the performance of the model. 5. Postprocessing: Visualizing the fitted model.\n\n#### Step 1: Split Input Data into Training and Test Sets\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train/test split\nnumTrain <- 20   # number of training instances\nnumTest <- numInstances - numTrain\n\nset.seed(123) # For reproducibility\n\ndata <- tibble(X = X, y = y)\n\nsplit_obj <- initial_split(data, prop = numTrain/numInstances)\n\n# Extract train and test data\ntrain_data <- training(split_obj)\ntest_data <- testing(split_obj)\n\n# Extract X_train, X_test, y_train, y_test\nX_train <- train_data$X\ny_train <- train_data$y\n\nX_test <- test_data$X\ny_test <- test_data$y\n```\n:::\n\n\n#### Step 2: Fit Regression Model to Training Set\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a linear regression model specification\nlin_reg_spec <- linear_reg() |> \n  set_engine(\"lm\")\n\n# Fit the model to the training data\nlin_reg_fit <- lin_reg_spec |> \n  fit(y ~ X, data = train_data)\n```\n:::\n\n\n#### Step 3: Apply Model to the Test Set\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply model to the test set\ny_pred_test <- predict(lin_reg_fit, new_data = test_data) |>\n  pull(.pred)\n```\n:::\n\n\n#### Step 4: Evaluate Model Performance on Test Set\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plotting true vs predicted values\nggplot() + \n  geom_point(aes(x = as.vector(y_test), y = y_pred_test), color = 'black') +\n  ggtitle('Comparing true and predicted values for test set') +\n  xlab('True values for y') +\n  ylab('Predicted values for y')\n```\n\n::: {.cell-output-display}\n![](rExercise4.2_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Prepare data for yardstick evaluation\neval_data <- tibble(\n  truth = as.vector(y_test),\n  estimate = y_pred_test\n)\n\n# Model evaluation\nrmse_value <- rmse(data = eval_data, truth = truth, estimate = estimate)\nr2_value <- rsq(eval_data, truth = truth, estimate = estimate)\n\ncat(\"Root mean squared error =\", sprintf(\"%.4f\", rmse_value$.estimate), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoot mean squared error = 1.0273 \n```\n:::\n\n```{.r .cell-code}\ncat('R-squared =', sprintf(\"%.4f\", r2_value$.estimate), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR-squared = 0.3911 \n```\n:::\n:::\n\n\n#### Step 5: Postprocessing\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Display model parameters\ncoef_values <- coef(lin_reg_fit$fit)  # Extract coefficients\nslope <- coef_values[\"X\"]\nintercept <- coef_values[\"(Intercept)\"]\n\ncat(\"Slope =\", slope, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSlope = -3.376872 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Intercept =\", intercept, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept = 0.9723522 \n```\n:::\n\n```{.r .cell-code}\n### Step 4: Postprocessing\n\n# Plot outputs\nggplot() +\n  geom_point(aes(x = as.vector(X_test), y = as.vector(y_test)), color = 'black') +\n  geom_line(aes(x = as.vector(X_test), y = y_pred_test), color = 'blue', linewidth = 1) +\n  ggtitle(sprintf('Predicted Function: y = %.2fX + %.2f', slope, intercept)) +\n  xlab('X') +\n  ylab('y')\n```\n\n::: {.cell-output-display}\n![](rExercise4.2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## **Effect of Correlated Attributes**\n\nIn this example, we illustrate how the presence of correlated attributes can affect the performance of regression models. Specifically, we create 4 additional variables, X2, X3, X4, and X5 that are strongly correlated with the previous variable X created in Section 5.1. The relationship between X and y remains the same as before. We then fit y against the predictor variables and compare their training and test set errors.\n\nFirst, we create the correlated attributes below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate the variables\nset.seed(1)\nX2 <- 0.5 * X + rnorm(numInstances, mean=0, sd=0.04)\nX3 <- 0.5 * X2 + rnorm(numInstances, mean=0, sd=0.01)\nX4 <- 0.5 * X3 + rnorm(numInstances, mean=0, sd=0.01)\nX5 <- 0.5 * X4 + rnorm(numInstances, mean=0, sd=0.01)\n\n# Create plots\nplot1 <- ggplot() +\n  geom_point(aes(X, X2), color='black') +\n  xlab('X') + ylab('X2') +\n  ggtitle(sprintf(\"Correlation between X and X2 = %.4f\", cor(X[-c((numInstances-numTest+1):numInstances)], X2[-c((numInstances-numTest+1):numInstances)])))\n\nplot2 <- ggplot() +\n  geom_point(aes(X2, X3), color='black') +\n  xlab('X2') + ylab('X3') +\n  ggtitle(sprintf(\"Correlation between X2 and X3 = %.4f\", cor(X2[-c((numInstances-numTest+1):numInstances)], X3[-c((numInstances-numTest+1):numInstances)])))\n\nplot3 <- ggplot() +\n  geom_point(aes(X3, X4), color='black') +\n  xlab('X3') + ylab('X4') +\n  ggtitle(sprintf(\"Correlation between X3 and X4 = %.4f\", cor(X3[-c((numInstances-numTest+1):numInstances)], X4[-c((numInstances-numTest+1):numInstances)])))\n\nplot4 <- ggplot() +\n  geom_point(aes(X4, X5), color='black') +\n  xlab('X4') + ylab('X5') +\n  ggtitle(sprintf(\"Correlation between X4 and X5 = %.4f\", cor(X4[-c((numInstances-numTest+1):numInstances)], X5[-c((numInstances-numTest+1):numInstances)])))\n\n# Combine plots into a 2x2 grid\ngrid.arrange(plot1, plot2, plot3, plot4, ncol=2)\n```\n\n::: {.cell-output-display}\n![](rExercise4.2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nNext, we create 4 additional versions of the training and test sets. The first version, X_train2 and X_test2 have 2 correlated predictor variables, X and X2. The second version, X_train3 and X_test3 have 3 correlated predictor variables, X, X2, and X3. The third version have 4 correlated variables, X, X2, X3, and X4 whereas the last version have 5 correlated variables, X, X2, X3, X4, and X5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split data into training and testing sets\ntrain_indices <- 1:(numInstances - numTest)\ntest_indices <- (numInstances - numTest + 1):numInstances\n\n# Create combined training and testing sets\nX_train2 <- cbind(X[train_indices], X2[train_indices])\nX_test2 <- cbind(X[test_indices], X2[test_indices])\n\nX_train3 <- cbind(X[train_indices], X2[train_indices], X3[train_indices])\nX_test3 <- cbind(X[test_indices], X2[test_indices], X3[test_indices])\n\nX_train4 <- cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices])\nX_test4 <- cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices])\n\nX_train5 <- cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices], X5[train_indices])\nX_test5 <- cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices], X5[test_indices])\n```\n:::\n\n\nBelow, we train 4 new regression models based on the 4 versions of training and test data created in the previous step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert matrices to tibbles for training\ntrain_data2 <- tibble(X1 = X_train2[,1], X2 = X_train2[,2], y = y_train)\ntrain_data3 <- tibble(X1 = X_train3[,1], X2 = X_train3[,2], X3 = X_train3[,3], y = y_train)\ntrain_data4 <- tibble(X1 = X_train4[,1], X2 = X_train4[,2], X3 = X_train4[,3], X4 = X_train4[,4], y = y_train)\ntrain_data5 <- tibble(X1 = X_train5[,1], X2 = X_train5[,2], X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5], y = y_train)\n\n# Train models\nregr2_spec <- linear_reg() %>% set_engine(\"lm\")\nregr2_fit <- regr2_spec %>% fit(y ~ X1 + X2, data = train_data2)\n\nregr3_spec <- linear_reg() %>% set_engine(\"lm\")\nregr3_fit <- regr3_spec %>% fit(y ~ X1 + X2 + X3, data = train_data3)\n\nregr4_spec <- linear_reg() %>% set_engine(\"lm\")\nregr4_fit <- regr4_spec %>% fit(y ~ X1 + X2 + X3 + X4, data = train_data4)\n\nregr5_spec <- linear_reg() %>% set_engine(\"lm\")\nregr5_fit <- regr5_spec %>% fit(y ~ X1 + X2 + X3 + X4 + X5, data = train_data5)\n```\n:::\n\n\nAll 4 versions of the regression models are then applied to the training and test sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert matrices to data.frames for predictions\nnew_train_data2 <- setNames(as.data.frame(X_train2), c(\"X1\", \"X2\"))\nnew_test_data2 <- setNames(as.data.frame(X_test2), c(\"X1\", \"X2\"))\n\nnew_train_data3 <- setNames(as.data.frame(X_train3), c(\"X1\", \"X2\", \"X3\"))\nnew_test_data3 <- setNames(as.data.frame(X_test3), c(\"X1\", \"X2\", \"X3\"))\n\nnew_train_data4 <- setNames(as.data.frame(X_train4), c(\"X1\", \"X2\", \"X3\", \"X4\"))\nnew_test_data4 <- setNames(as.data.frame(X_test4), c(\"X1\", \"X2\", \"X3\", \"X4\"))\n\nnew_train_data5 <- setNames(as.data.frame(X_train5), c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"))\nnew_test_data5 <- setNames(as.data.frame(X_test5), c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"))\n\n# Predictions\ny_pred_train2 <- predict(regr2_fit, new_data = new_train_data2)\ny_pred_test2 <- predict(regr2_fit, new_data = new_test_data2)\n\ny_pred_train3 <- predict(regr3_fit, new_data = new_train_data3)\ny_pred_test3 <- predict(regr3_fit, new_data = new_test_data3)\n\ny_pred_train4 <- predict(regr4_fit, new_data = new_train_data4)\ny_pred_test4 <- predict(regr4_fit, new_data = new_test_data4)\n\ny_pred_train5 <- predict(regr5_fit, new_data = new_train_data5)\ny_pred_test5 <- predict(regr5_fit, new_data = new_test_data5)\n```\n:::\n\n\nFor postprocessing, we compute both the training and test errors of the models. We can also show the resulting model and the sum of the absolute weights of the regression coefficients, i.e., $\\sum_{j=0}^d |w_j|$, where $d$ is the number of predictor attributes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract coefficients and intercepts\nget_coef <- function(model) {\n  coef <- coefficients(model$fit)\n  coef\n}\n\n# Calculate RMSE\ncalculate_rmse <- function(actual, predicted) {\n  rmse <- sqrt(mean((actual - predicted)^2))\n  rmse\n}\n\nresults <- tibble(\n  Model = c(sprintf(\"%.2f X + %.2f\", get_coef(regr2_fit)['X1'], get_coef(regr2_fit)['(Intercept)']),\n            sprintf(\"%.2f X + %.2f X2 + %.2f\", get_coef(regr3_fit)['X1'], get_coef(regr3_fit)['X2'], get_coef(regr3_fit)['(Intercept)']),\n            sprintf(\"%.2f X + %.2f X2 + %.2f X3 + %.2f\", get_coef(regr4_fit)['X1'], get_coef(regr4_fit)['X2'], get_coef(regr4_fit)['X3'], get_coef(regr4_fit)['(Intercept)']),\n            sprintf(\"%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f\", get_coef(regr5_fit)['X1'], get_coef(regr5_fit)['X2'], get_coef(regr5_fit)['X3'], get_coef(regr5_fit)['X4'], get_coef(regr5_fit)['(Intercept)'])),\n  \n  Train_error = c(calculate_rmse(y_train, y_pred_train2$.pred),\n                  calculate_rmse(y_train, y_pred_train3$.pred),\n                  calculate_rmse(y_train, y_pred_train4$.pred),\n                  calculate_rmse(y_train, y_pred_train5$.pred)),\n  \n  Test_error = c(calculate_rmse(y_test, y_pred_test2$.pred),\n                 calculate_rmse(y_test, y_pred_test3$.pred),\n                 calculate_rmse(y_test, y_pred_test4$.pred),\n                 calculate_rmse(y_test, y_pred_test5$.pred)),\n  \n  Sum_of_Absolute_Weights = c(sum(abs(get_coef(regr2_fit))),\n                              sum(abs(get_coef(regr3_fit))),\n                              sum(abs(get_coef(regr4_fit))),\n                              sum(abs(get_coef(regr5_fit))))\n)\n\n# Plotting\nggplot(results, aes(x = Sum_of_Absolute_Weights)) +\n  geom_line(aes(y = Train_error, color = \"Train error\"), linetype = \"solid\") +\n  geom_line(aes(y = Test_error, color = \"Test error\"), linetype = \"dashed\") +\n  labs(x = \"Sum of Absolute Weights\", y = \"Error rate\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](rExercise4.2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  Model                            Train_error Test_error Sum_of_Absolute_Weig…¹\n  <chr>                                  <dbl>      <dbl>                  <dbl>\n1 -0.53 X + -1.05                         1.33       1.30                   3.64\n2 0.34 X + 20.99 X2 + -1.05               1.26       1.35                  62.8 \n3 0.07 X + 22.72 X2 + -66.35 X3 +…        1.18       1.49                 137.  \n4 -1.83 X + 22.46 X2 + -63.04 X3 …        1.17       1.53                 178.  \n# ℹ abbreviated name: ¹​Sum_of_Absolute_Weights\n```\n:::\n:::\n\n\nThe results above show that the first model, which fits y against X only, has the largest training error, but smallest test error, whereas the fifth model, which fits y against X and other correlated attributes, has the smallest training error but largest test error. This is due to a phenomenon known as model overfitting, in which the low training error of the model does not reflect how well the model will perform on previously unseen test instances. From the plot shown above, observe that the disparity between the training and test errors becomes wider as the sum of absolute weights of the model (which represents the model complexity) increases. Thus, one should control the complexity of the regression model to avoid the model overfitting problem.\n\n## **Ridge Regression**\n\nRidge regression is a variant of MLR designed to fit a linear model to the dataset by minimizing the following regularized least-square loss function:\n\n$$\nL_{\\textrm{ridge}}(y,f(X,w)) = \\sum_{i=1}^N \\|y_i - X_iw - w_0\\|^2 + \\alpha \\bigg[\\|w\\|^2 + w_0^2 \\bigg],\n$$\n\nwhere $\\alpha$ is the hyperparameter for ridge regression. Note that the ridge regression model reduces to MLR when $\\alpha = 0$. By increasing the value of $\\alpha$, we can control the complexity of the model as will be shown in the example below.\n\nIn the example shown below, we fit a ridge regression model to the previously created training set with correlated attributes. We compare the results of the ridge regression model against those obtained using MLR.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert to data frame\ntrain_data <- tibble(y = y_train, X_train5)\ntest_data <- tibble(y = y_test, X_test5)\n\n# Set up a Ridge regression model specification\nridge_spec <- linear_reg(penalty = 0.4, mixture = 1) %>% \n  set_engine(\"glmnet\")\n\n# Fit the model\nridge_fit <- ridge_spec %>% \n  fit(y ~ ., data = train_data)\n\n# Make predictions\ny_pred_train_ridge <- predict(ridge_fit, new_data = train_data)$.pred\ny_pred_test_ridge <- predict(ridge_fit, new_data = test_data)$.pred\n\n\n# Make predictions\ny_pred_train_ridge <- predict(ridge_fit, new_data = train_data)$.pred\ny_pred_test_ridge <- predict(ridge_fit, new_data = train_data)$.pred\n\n# Calculate RMSE\ncalculate_rmse <- function(actual, predicted) {\n  rmse <- sqrt(mean((actual - predicted)^2))\n  rmse\n}\n\n# Extract coefficients\nridge_coef <- coefficients(ridge_fit$fit)\n\nmodel6 <- sprintf(\"%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f\", \n                 ridge_coef[2], ridge_coef[3], ridge_coef[4], \n                 ridge_coef[5], ridge_coef[6], ridge_coef[1])\n\nvalues6 <- tibble(\n  Model = model6,\n  Train_error = calculate_rmse(y_train, y_pred_train_ridge),\n  Test_error = calculate_rmse(y_test, y_pred_test_ridge),\n  Sum_of_Absolute_Weights = sum(abs(ridge_coef))\n)\n\n# Combining the results\nfinal_results <- bind_rows(results, values6)\n\nfinal_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 4\n  Model                            Train_error Test_error Sum_of_Absolute_Weig…¹\n  <chr>                                  <dbl>      <dbl>                  <dbl>\n1 -0.53 X + -1.05                         1.33       1.30                   3.64\n2 0.34 X + 20.99 X2 + -1.05               1.26       1.35                  62.8 \n3 0.07 X + 22.72 X2 + -66.35 X3 +…        1.18       1.49                 137.  \n4 -1.83 X + 22.46 X2 + -63.04 X3 …        1.17       1.53                 178.  \n5 0.00 X + 0.00 X2 + 0.00 X3 + 0.…        1.35       1.29                8581.  \n# ℹ abbreviated name: ¹​Sum_of_Absolute_Weights\n```\n:::\n:::\n\n\nBy setting an appropriate value for the hyperparameter, �, we can control the sum of absolute weights, thus producing a test error that is quite comparable to that of MLR without the correlated attributes.\n\n## **Lasso Regression**\n\nOne of the limitations of ridge regression is that, although it was able to reduce the regression coefficients associated with the correlated attributes and reduce the effect of model overfitting, the resulting model is still not sparse. Another variation of MLR, called lasso regression, is designed to produce sparser models by imposing an $l_1$ regularization on the regression coefficients as shown below:\n\n$$\nL_{\\textrm{lasso}}(y,f(X,w)) = \\sum_{i=1}^N \\|y_i - X_iw - w_0\\|^2 + \\alpha \\bigg[ \\|w\\|_1 + |w_0|\\bigg]\n$$\n\nThe example code below shows the results of applying lasso regression to the previously used correlated dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the lasso specification\nlasso_spec <- linear_reg(penalty = 0.02, mixture = 1) %>% \n  set_engine(\"glmnet\")\n\n# Ensure the data is combined correctly\ntrain_data <- tibble(y = y_train, X1 = X_train5[,1], X2 = X_train5[,2], \n                     X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5])\n\n# Fit the model\nlasso_fit <- lasso_spec %>%\n  fit(y ~ ., data = train_data)\n\n# Extract coefficients\nlasso_coefs <- lasso_fit$fit$beta[,1]\n\n# Predictions\ny_pred_train_lasso <- predict(lasso_fit, new_data = train_data)$.pred\ny_pred_test_lasso <- predict(lasso_fit, new_data = tibble(X1 = X_test5[,1], X2 = X_test5[,2], \n                                                          X3 = X_test5[,3], X4 = X_test5[,4], X5 = X_test5[,5]))$.pred\n\n# Create the model string\nmodel7 <- sprintf(\"%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f\", \n                  lasso_coefs[2], lasso_coefs[3], lasso_coefs[4], \n                  lasso_coefs[5], lasso_coefs[6], lasso_fit$fit$a0[1])\n\nvalues7 <- c(model7, \n             sqrt(mean((y_train - y_pred_train_lasso)^2)),\n             sqrt(mean((y_test - y_pred_test_lasso)^2)),\n             sum(abs(lasso_coefs[-1])) + abs(lasso_fit$fit$a0[1]))\n\n# Make the results tibble\nlasso_results <- tibble(Model = \"Lasso\",\n                        `Train error` = values7[2], \n                        `Test error` = values7[3], \n                        `Sum of Absolute Weights` = values7[4])\n\nlasso_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  Model `Train error`    `Test error`     `Sum of Absolute Weights`\n  <chr> <chr>            <chr>            <chr>                    \n1 Lasso 1.22083472815552 1.36447668533408 0.750560758224512        \n```\n:::\n:::\n\n\nObserve that the lasso regression model sets the coefficients for the correlated attributes, X2, X3, X4, and X5 to exactly zero unlike the ridge regression model. As a result, its test error is significantly better than that for ridge regression.\n\n## **Hyperparameter Selection via Cross-Validation**\n\nWhile both ridge and lasso regression methods can potentially alleviate the model overfitting problem, one of the challenges is how to select the appropriate hyperparameter value, $\\alpha$. In the examples shown below, we demonstrate examples of using a 5-fold cross-validation method to select the best hyperparameter of the model. More details about the model selection problem and cross-validation method are described in Chapter 3 of the book.\n\nIn the first sample code below, we vary the hyperparameter $\\alpha$ for ridge regression to a range between 0.2 and 1.0. Using the `RidgeCV()` function, we can train a model with 5-fold cross-validation and select the best hyperparameter value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine training data\ny_train <- as.vector(y_train)\n\ntrain_data <- tibble(y = y_train, X1 = X_train5[,1], X2 = X_train5[,2], \n                     X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5])\n\n# Define recipe\nrecipe_obj <- recipe(y ~ ., data = train_data) %>%\n  step_normalize(all_predictors()) |>\n  prep()\n\n# Define the ridge specification\nridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>% \n  set_engine(\"glmnet\")\n\n# Ridge workflow\nridge_wf <- workflow() |>\n  add_model(ridge_spec) |>\n  add_recipe(recipe_obj)\n\n# Grid of alphas\nalphas <- tibble(penalty = c(0.2, 0.4, 0.6, 0.8, 1.0))\n\n# Tune\ntune_results <- \n  ridge_wf |>\n  tune_grid(\n  resamples = bootstraps(train_data, times = 5),\n  grid = alphas\n)\n\n\n# Extract best parameters\nbest_params <- tune_results %>% select_best(\"rmse\")\n\n# Refit the model\nridge_fit <- ridge_spec %>%\n  finalize_model(best_params) %>%\n  fit(y ~ ., data = train_data)\n\n# Extract coefficients\nridge_coefs <- ridge_fit$fit$beta[,1]\n\n# Predictions\ny_pred_train_ridge <- predict(ridge_fit, new_data = train_data)$.pred\ny_pred_test_ridge <- predict(ridge_fit, new_data = tibble(X1 = X_test5[,1], X2 = X_test5[,2], \n                                                          X3 = X_test5[,3], X4 = X_test5[,4], X5 = X_test5[,5]))$.pred\n\n# Create the model string\nmodel6 <- sprintf(\"%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f\", \n                  ridge_coefs[2], ridge_coefs[3], ridge_coefs[4], \n                  ridge_coefs[5], ridge_coefs[6], ridge_fit$fit$a0[1])\n\nvalues6 <- c(model6, \n             sqrt(mean((y_train - y_pred_train_ridge)^2)),\n             sqrt(mean((y_test - y_pred_test_ridge)^2)),\n             sum(abs(ridge_coefs[-1])) + abs(ridge_fit$fit$a0[1]))\n\n# Make the results tibble\nridge_results <- tibble(Model = \"RidgeCV\",\n                        `Train error` = values6[2], \n                        `Test error` = values6[3], \n                        `Sum of Absolute Weights` = values6[4])\n\ncat(\"Selected alpha =\", best_params$penalty, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSelected alpha = 1 \n```\n:::\n\n```{.r .cell-code}\nall_results <- bind_rows(results, ridge_results)\nall_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 7\n  Model Train_error Test_error Sum_of_Absolute_Weig…¹ `Train error` `Test error`\n  <chr>       <dbl>      <dbl>                  <dbl> <chr>         <chr>       \n1 -0.5…        1.33       1.30                   3.64 <NA>          <NA>        \n2 0.34…        1.26       1.35                  62.8  <NA>          <NA>        \n3 0.07…        1.18       1.49                 137.   <NA>          <NA>        \n4 -1.8…        1.17       1.53                 178.   <NA>          <NA>        \n5 Ridg…       NA         NA                     NA    1.3309131350… 1.295979736…\n# ℹ abbreviated name: ¹​Sum_of_Absolute_Weights\n# ℹ 1 more variable: `Sum of Absolute Weights` <chr>\n```\n:::\n:::\n\n\nIn this next example, we illustrate how to apply cross-validation to select the best hyperparameter value for fitting a lasso regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\n# Ensure y_train is a vector\ny_train <- as.vector(y_train)\n\n# Combine training data\ntrain_data <- tibble(y = y_train, X1 = X_train5[,1], X2 = X_train5[,2], \n                     X3 = X_train5[,3], X4 = X_train5[,4], X5 = X_train5[,5])\n\n# Define recipe\nrecipe_obj_lasso <- recipe(y ~ ., data = train_data) %>%\n  step_normalize(all_predictors()) |>\n  prep()\n\n# Define the lasso specification\nlasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\")\n\n# Lasso workflow\nlasso_wf <- workflow() |>\n  add_recipe(recipe_obj_lasso)\n\n# Lasso fit\nlasso_fit <- lasso_wf |>\n  add_model(lasso_spec) |>\n  fit(data = train_data)\n\n# Grid of alphas for Lasso\nlambda_grid <- grid_regular(penalty(), levels = 50)\n\n# Tune\ntune_results_lasso <- \n  tune_grid(lasso_wf |> add_model(lasso_spec),\n  resamples = bootstraps(train_data, times = 5),\n  grid = lambda_grid\n)\n\n# Extract best parameters for Lasso\nbest_params_lasso <- tune_results_lasso %>% select_best(\"rmse\")\n\n# Refit the model using Lasso\nlasso_fit <- lasso_spec %>%\n  finalize_model(best_params_lasso) %>%\n  fit(y ~ ., data = train_data)\n\n# Extract coefficients\nlasso_coefs <- lasso_fit$fit$beta[,1]\n\n# Predictions using Lasso\ny_pred_train_lasso <- predict(lasso_fit, new_data = train_data)$.pred\ny_pred_test_lasso <- predict(lasso_fit, new_data = tibble(X1 = X_test5[,1], X2 = X_test5[,2], \n                                                          X3 = X_test5[,3], X4 = X_test5[,4], X5 = X_test5[,5]))$.pred\n\n# Create the model string for Lasso\nmodel7 <- sprintf(\"%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f\", \n                  lasso_coefs[2], lasso_coefs[3], lasso_coefs[4], \n                  lasso_coefs[5], lasso_coefs[6], lasso_fit$fit$a0[1])\n\nvalues7 <- c(model7, \n             sqrt(mean((y_train - y_pred_train_lasso)^2)),\n             sqrt(mean((y_test - y_pred_test_lasso)^2)),\n             sum(abs(lasso_coefs[-1])) + abs(lasso_fit$fit$a0[1]))\n\n# Make the results tibble for Lasso\nlasso_results <- tibble(Model = \"LassoCV\",\n                        `Train error` = values7[2], \n                        `Test error` = values7[3], \n                        `Sum of Absolute Weights` = values7[4])\n\ncat(\"Selected alpha for Lasso =\", best_params_lasso$penalty, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSelected alpha for Lasso = 0.6250552 \n```\n:::\n\n```{.r .cell-code}\nlasso_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  Model   `Train error`    `Test error`     `Sum of Absolute Weights`\n  <chr>   <chr>            <chr>            <chr>                    \n1 LassoCV 1.34525910987747 1.28985807470116 0.750560758224512        \n```\n:::\n:::\n\n\n## **Summary**\n\nThis section presents example Python code for fitting linear regression models to a dataset. We also illustrate the problem of model overfitting and shows two alternative methods, called ridge and lasso regression, that can help alleviate such problem. While the model overfitting problem shown here is illustrated in the context of correlated attributes, the problem is more general and may arise due to other factors such as noise and other exceptional values in the data.\n",
    "supporting": [
      "rExercise4.2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}